# LLMs Tools & Research Projects
The repository contains a list of ready-to-use AI Tools, Open Sources, and Research Projects \
Apart from LLMs, you can find here new AI research from other areas such as Computer Vision, etc.\
Welcome to contribute.

## Large Language Models (LLMs) and Chatbots
[ChatGPT Prompt Engineering for Developers | Andrew Ng | Isa Fulford | Course](https://learn.deeplearning.ai/chatgpt-prompt-eng) \
[The Inside Story of ChatGPT’s Astonishing Potential | Greg Brockman | TED](https://youtu.be/C_78DM8fG6E) \
[State of GPT | Andrej Karpathy](https://youtu.be/bZQun8Y4L2A)\
[Visualization: The Rise and Rise of A.I. LLMs & their associated bots like ChatGPT](https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt)
||Google|OpenAI|Meta|EleutherAI|DeepMind|Stability AI||
:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:
2023|[PALM-2](https://ai.google/discover/palm2)<br>[Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/)|[GPT-4](https://openai.com/product/gpt-4)|[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)|[Pythia](https://github.com/EleutherAI/pythia)||[Vicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot), [Demo: HF](https://huggingface.co/spaces/CarperAI/StableVicuna)<br>[StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)||
2022|[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)|[ChatGPT](https://openai.com/blog/chatgpt)||GPT-NeoX|||[Bloom](https://huggingface.co/bigscience/bloom)|
2021|[LaMDA](https://blog.google/technology/ai/lamda/)||||||

- [LAION](https://laion.ai/) - Large-scale Artificial Intelligence Open Network
- [Open LLM Leaderboard: HF](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) - aims to track, rank and evaluate LLMs and chatbots as they are released
- [Falcon](https://falconllm.tii.ae/), [Demo: HF](https://huggingface.co/blog/falcon) - a new family of state-of-the-art language models created by the Technology Innovation Institute in Abu Dhabi, and released under the Apache 2.0 license
- [Dalai](https://cocktailpeanut.github.io/dalai/#/), [Repo](https://github.com/cocktailpeanut/dalai) - run LLaMA and Alpaca on your computer
- [LLaMAChat](https://llamachat.app/) - allows you to chat with LLaMa, Alpaca and GPT4All models1 all running locally on your Mac
- [GPT4All](https://gpt4all.io/index.html), [Repo](https://github.com/nomic-ai/gpt4all) - an open-source assistant-style large language models that run locally on your CPU. No GPU or internet required
- [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [Repo](https://github.com/tatsu-lab/stanford_alpaca) - a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce
- [Chatbot Arena ](https://chat.lmsys.org/) - a scalable and gamified evaluation of LLMs via crowdsourcing and Elo rating systems. Chat with two anonymous models side-by-side and vote for which one is better
- [AI Playground](https://play.vercel.ai/) - you can input a prompt, pick different LLMS, and compare two side by side
- [MultiModal-GPT](https://github.com/open-mmlab/Multimodal-GPT) - a vision and language model for multi-round dialogue with humans; the model is fine-tuned from OpenFlamingo, with LoRA added in the cross-attention and self-attention parts of the language model
- [Pi](https://heypi.com/talk) - this bot is designed to be more of a personal assistant
- [ChatwithData.ai]() - AI tool that lets you extract valuable insights and information from data files effortlessly
- [Open Assistant](https://open-assistant.io/) - a completely open-source ChatGPT alternative
- [HuggingChat](https://huggingface.co/chat/) - first open-source alternative to ChatGPT Powered by Open Assistant's latest model
- [Claude](https://www.anthropic.com/product) - a next-generation AI assistant for your tasks, no matter the scale
- [ChatPDF](https://www.chatpdf.com/) - chat with any PDF
- [PdfGPT](https://pdfgpt.io/) - is a tool where you can upload pdf and get summaries, answers to your questions by OpenAI
- [Baize](https://github.com/project-baize/baize-chatbot) - is an open-source chat model trained with LoRA. It uses 100k dialogs generated by letting ChatGPT chat with itself. We also use Alpaca's data to improve its performance
- [MiniGPT-4: Enhancing Vision-language Understanding with Advanced LLM](https://minigpt-4.github.io/) - upload an image, and then use chat to identify what's in the picture and learn more about it
- [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://chameleon-llm.github.io/) - is a compositional reasoning framework designed to enhance large language models (LLMs) and overcome their inherent limitations, such as outdated information and lack of precise reasoning
- [LLaVA: Large Language and Vision Assistant](https://llava-vl.github.io/) - visual instruction tuning towards large language and vision models with GPT-4 level capabilities
- [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt) - connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting
- [LLaMA-Adapter](https://github.com/zrrskywalker/llama-adapter) - a lightweight adaption method for fine-tuning instruction-following LLaMA models 
- [OpenICL](https://github.com/Shark-NLP/OpenICL) - a new open-source toolkit for in-context learning and LLM evaluation; supports various state-of-the-art retrieval and inference methods, tasks, and zero-/few-shot evaluation of LLMs

## Text-to-image
### Tools
|Text-to-image|Text-to-video|Games|Brand|
:-:|:-:|:-:|:-:
[Midjourney](https://www.midjourney.com/)|[GENMO](https://alpha.genmo.ai/)|[Leonardo.Ai](https://leonardo.ai/) - Assets|[Flair](https://flair.ai/)
[Adobe Firefly](https://firefly.adobe.com/)||[Dreamlab](https://dreamlab.gg/) - Animated Sprites|[Logolivery](https://logolivery.ai/)|
[Catbird](https://www.catbird.ai/)||[Didimo](https://www.didimo.co/)|
[BlueWillow](https://www.bluewillow.ai/)||[Scenario](https://www.scenario.com/) - Assets|
[Lexica](https://lexica.art/)||[Skybox](https://skybox.blockadelabs.com/) - World-building|
[Craiyon](https://www.craiyon.com/)||[lumine AI](https://ilumine.ai/)|

- [StyleDrop: Text-To-Image Generation in Any Style](https://styledrop.github.io/) - that enables the generation of images that faithfully follow a specific style, powered by Muse, a text-to-image generative vision transformer, by Google Research
- [Stable Diffusion XL](https://stability.ai/stable-diffusion), [DreamStudio](https://dreamstudio.ai/) - create descriptive images with shorter prompts and generate worlds within images
- [DeepFloyd IF](https://stability.ai/blog/deepfloyd-if-text-to-image-model), [Repo](https://github.com/deep-floyd/IF), [Demo: HuggingFace](https://huggingface.co/spaces/DeepFloyd/IF),- is a state-of-the-art text-to-image model released on a non-commercial, research-permissible license that provides an opportunity for research labs to examine and experiment with advanced text-to-image generation approaches. In line with other Stability AI models, Stability AI intends to release a DeepFloyd IF model fully open source at a future date, by Stability AI, together with its multimodal AI research lab DeepFloyd
- [CLIP](https://openai.com/research/clip) - a neural network which efficiently learns visual concepts from natural language supervision, by OpenAI
- [DALL·E 2](https://openai.com/dall-e-2) - an AI system that can create realistic images and art from a description in natural language, by OpenAI
- [DALL·E](https://openai.com/research/dall-e) - a version of GPT-3 trained to generate images from text descriptions, using a dataset of text–image pairs, by OpenAI
- [Parti](https://sites.research.google/parti/) - Pathways Autoregressive Text-to-Image model, an autoregressive text-to-image generation model that achieves high-fidelity photorealistic image generation and supports content-rich synthesis involving complex compositions and world knowledge, by Google Research
- [Imagen](https://imagen.research.google/) - a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding, by Google Research, Brain Team
- [Muse](https://muse-model.github.io/) - Text-To-Image Generation via Masked Generative Transformers, a fast, state-of-the-art text-to-image generation and editing model, by Google Research
- [InstructPix2Pix Learning to Follow Image Editing Instructions](https://www.timothybrooks.com/instruct-pix2pix/) - a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image
- [Composer](https://damo-vilab.github.io/composer-page/) - a large (5 billion parameters) controllable diffusion model trained on billions of (text, image) pairs. It can exponentially expand the control space through composition, leading to an enormous number of ways to generate and manipulate images, i.e., making the infinite use of finite means
- [GigaGAN: Large-scale GAN for Text-to-Image Synthesis](https://mingukkang.github.io/GigaGAN/) - changing texture with prompting, changing style with prompting
- [OpenAGI: When LLM Meets Domain Experts](https://github.com/agiresearch/OpenAGI)

## Multi-modal
- [ImageBind](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/), [Demo](https://imagebind.metademolab.com/demo), [Repo](https://github.com/facebookresearch/ImageBind) - Image->Audio, Audio->Image, Text->Image&Audio, Aidio&Image->Image, Audio->Generated Image by Meta
- [Clipdrop](https://clipdrop.co/) - by StabilityAI
  - [Image Upscaler](https://clipdrop.co/image-upscaler), [Blog](https://stability.ai/blog/stability-ai-releases-image-upscaling-api) - an AI-powered tool that increases the size of any image without compromising its sharpness
  - [Uncrop](https://clipdrop.co/uncrop), [Blog](https://stability.ai/blog/clipdrop-launches-uncrop-the-ultimate-aspect-ratio-editor) - a tool helps to uncrop your photos to any image format
  - [Reimagine](https://clipdrop.co/stable-diffusion-reimagine), [Blog](https://stability-ai.squarespace.com/blog/stability-ai-clipdrop-launches-reimagine-xl) - a tool helps to create multiple variations from a single image with stable diffusion
  - [Stable Diffusion XL](https://clipdrop.co/stable-diffusion) - generate images from text
- [Kaiber](https://kaiber.ai/) - 
  - Audioreactivity: upload a song, add a touch of your artistic style, and let our audio analysis technology bring your beats to life
  - Animation: start with a few words or upload your own image, and watch your ideas turn into captivating visuals
  - Transform: upload your videos and effortlessly transform them into entirely new styles and aesthetics
- [GEN-1](https://runwayml.com/ai-magic-tools/gen-1/), [Research](https://research.runwayml.com/gen1) - use words and images to generate new videos out of existing ones by Runway: [AI-Magic-Tools](https://runwayml.com/ai-magic-tools/)
- [GEN-2](https://runwayml.com/ai-magic-tools/gen-2/), [Research](https://research.runwayml.com/gen2) - create videos in any style you can imagine with Text to Video generation by Runway: [AI-Magic-Tools](https://runwayml.com/ai-magic-tools/)
  - Mode 01: Text to Video: Synthesize videos in any style you can imagine using nothing but a text prompt. If you can say it, now you can see it
  - Mode 02: Text + Image to Video: Generate a video using a driving image and a text prompt
  - Mode 03: Image to Video: Generate video using just a driving image (Variations Mode)
  - Mode 04: Stylization: Transfer the style of any image or prompt to every frame of your video
  - Mode 05: Storyboard: Turn mockups into fully stylized and animated renders
  - Mode 06: Mask: Isolate subjects in your video and modify them with simple text prompts
  - Mode 07: Render: Turn untextured renders into realistic outputs by applying an input image or prompt
  - Mode 08: Customization: Unleash the full power of Gen-2 by customizing the model for even higher fidelity results
- [MONSTER API](https://monsterapi.ai/)
  - text-to-image: a latent text-to-image diffusion model capable of generating photo-realistic images conditioned on text descriptions
  - image-to-image: a latent diffusion model capable of generating photo-realistic generating image-to-image translations guided by a text prompt
  - instruct-pix2pix: a model enables fast and effective image editing based on simple instructions
- [Prismer: A Vision-Language Model with Multi-Modal Experts](https://shikun.io/projects/prismer) - a data- and parameter-efficient vision-language model that leverages an ensemble of diverse, pre-trained domain experts

## Typography
- [ControlNet](https://huggingface.co/DionTimmer/controlnet_qrcode), [Demo: HF](https://huggingface.co/spaces/huggingface-projects/QR-code-AI-art-generator), [How to make a QR code with Stable Diffusion](https://stable-diffusion-art.com/qr-code/) - QR Code Conditioned ControlNet Models for Stable Diffusion. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape
- [Word-As-Image for Semantic Typography](https://wordasimage.github.io/Word-As-Image-Page/) - A few examples of our Word-As-Image illustrations in various fonts and for different textual concept. The semantically adjusted letters are created completely automatically using our method, and can then be used for further creative design as we illustrate here
- [DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion](https://ds-fusion.github.io/) - create artistic typography automatically, a novel method to automatically generate artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word, while ensuring that the output remains readable

## Images
- [Plug-and-Play](https://pnp-diffusion.github.io/), [Code](https://github.com/MichalGeyer/plug-and-play) - a large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, allowing us to synthesize diverse images that convey highly complex visual concepts
- [Real-Time Neural Appearance Models](https://research.nvidia.com/labs/rtr/neural_appearance_models/) - a complete system for real-time rendering of scenes with complex appearance previously reserved for offline use, by NVIDIA
- [Designer](https://designer.microsoft.com/), [Microsoft Designer expands preview with new AI design features](https://www.microsoft.com/en-us/microsoft-365/blog/2023/04/27/microsoft-designer-expands-preview-with-new-ai-design-features/) by Microsoft. Designer has all the tools you’d expect, plus a few AI superpowers. Generate stunning designs and original images just by typing what you want. Get writing assistance and automatic layout suggestions for anything you add. Designer can even propose captions and hashtags to make social media sharing effortless
- [Scribble Diffusion](https://scribblediffusion.com/) - turn your sketch into a refined image using AI
- [StudioGPT](https://www.latentlabs.art/) - a tool for reimagining an existing image

## Computer Vision
- [I-JEPA](https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa), [Repo](https://github.com/facebookresearch/ijepa) - Image Joint Embedding Predictive Architecture is a method for self-supervised learning. At a high level, I-JEPA predicts the representations of part of an image from the representations of other parts of the same image
- [Visual Prompting](https://landing.ai/What-is-visual-prompting/) - an innovative approach that takes text prompting, used in applications such as ChatGPT, to computer vision
- [Tracking Everything Everywhere All at Once](https://omnimotion.github.io/) - a new test-time optimization method for estimating dense and long-range motion from a video sequence
- [Track-Anything](https://github.com/gaomingqi/Track-Anything) - a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only
- [Segment Anything Model (SAM)](https://segment-anything.com/) - a new AI model from Meta AI that can "cut out" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training. [Blog: Introducing Segment Anything](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/), [Repository](https://github.com/facebookresearch/segment-anything)
- [DINOv2](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/) - a new method for training high-performance CV models, state-of-the-art CV models with self-supervised learning
- [Behind the Scenes: Density Fields for Single View Reconstruction](https://fwmb.github.io/bts/) - a neural network that predicts an implicit density field from a single image

## Video & Animation
- [Klap](https://klap.app/) - a tool that analyzes the video and finds short clips
- [Lalamu](https://lalamu.studio/demo) - low-quality video lip sync with preselected videos/video templates (take clips from videos, give the video new audio, and then the lips will sync up to that new audio within the video)
- [D-ID](https://www.d-id.com/) - uses generative AI to create customized videos featuring talking avatars at a touch of a button for businesses and creators.
- [Rooms.xyz](https://rooms.xyz) - create & remix interactive rooms from your browser
- [REVELxyz](https://www.revel.xyz/animai) - a tool for creating Animated Avatars from a single photo
- [ANIMATED DRAWINGS](https://sketch.metademolab.com/) - a tool that brings children's drawings to life, by animating characters to move around, by MetaAI
- [RERENDER A VIDEO](https://anonymous-31415926.github.io/), [Demo: HF](https://huggingface.co/spaces/Anonymous-sub/Rerender) - a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos
- Roop, [Repo](https://github.com/s0md3v/roop) - take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training
- [Text2Performer](https://yumingj.github.io/projects/Text2Performer.html) - Text-Driven Human Video Generation, where a video sequence is synthesized from texts describing the appearance and motions of a target performer
- [DragGAN](https://vcai.mpi-inf.mpg.de/projects/DragGAN/) - way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc
- [In-N-Out: Face Video Inversion and Editing with Volumetric Decomposition](https://in-n-out-3d.github.io/) - our core idea is to represent the face in a video using two neural radiance fields, one for in-distribution and the other for out-of-distribution data, and compose them together for reconstruction
- [High-Resolution Video Synthesis with Latent Diffusion Models](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) - Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space, by NVIDIA

## 3D
 - [AvatarBooth](https://zeng-yifei.github.io/avatarbooth_page/) - is a text-to-3D model. It creates an animatable 3D model with your word description. Also, it can generate customized model with 4~6 photos from your phone or a character design generated from diffusion model
 - [Infinigen](https://infinigen.org/), [Code](https://github.com/princeton-vl/infinigen) - a procedural generator of 3D scenes, creating depth maps and labeling every aspect of the world it generates, by Princeton Vision & Learning Lab
 - [USD - Universal Scene Description](https://developer.nvidia.com/usd) - an open and extensible framework and ecosystem for describing, composing, simulating and collaborating within 3D worlds, originally developed by Pixar Animation Studios
 - [Shap-E: Demo](https://huggingface.co/spaces/hysts/Shap-E), [Repo](https://github.com/openai/shap-e) - a conditional generative model for 3D assets, by OpenAI
 - [Neural Kernel Surface Reconstruction](https://research.nvidia.com/labs/toronto-ai/NKSR/), [Code](https://github.com/nv-tlabs/nksr)- a novel method for reconstructing a 3D implicit surface from a large-scale, sparse, and noisy point, by NVIDIA
 - [Neuralangelo](https://research.nvidia.com/labs/dir/neuralangelo/) - a framework for high-fidelity 3D surface reconstruction from RGB video captures. Using ubiquitous mobile devices, we enable users to create digital twins of both object-centric and large-scale real-world scenes with highly detailed 3D geometry, by NVIDIA
 - [Rodin Diffusion](https://3d-avatar-diffusion.microsoft.com/) - a Generative Model for Sculpting 3D Digital Avatars, by Microsoft
 - [Text2NeRF](https://eckertzhang.github.io/Text2NeRF.github.io/) - a text-driven 3D scene generation framework, combines the neural radiance field (NeRF) and a pre-trained text-to-image diffusion model to generate diverse view-consistent indoor and outdoor 3D scenes from natural language descriptions
- [S-NeRF](https://ziyang-xie.github.io/s-nerf/) - a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly
- [Mip-NeRF 360](https://jonbarron.info/mipnerf360/) - Unbounded Anti-Aliased Neural Radiance Fields, an extension of mip-NeRF that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes
- [3D-aware Conditional Image Synthesis](https://www.cs.cmu.edu/~pix2pix3D/) - a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model synthesizes a photo from different viewpoints
- [Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior](https://make-it-3d.github.io/) - can create high-fidelity 3D content from only a single image
- [Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](https://lukashoel.github.io/text-to-room/) - generates textured 3D meshes from a given text prompt using 2D text-to-image models
- [OmniObject3D](https://omniobject3d.github.io/) - a large vocabulary 3D object dataset with massive high-quality real-scanned 3D objects to facilitate the development of 3D perception, reconstruction, and generation in the real world

## Robotics
- [TidyBot](https://tidybot.cs.princeton.edu/) - personalized Robot Assistance with Large Language Models
- [Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning](https://sites.google.com/view/op3-soccer) - by OP3 Soccer Team, DeepMind
- [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/) - embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts
- [Scaling Robot Learning with Semantically Imagined Experience](https://diffusion-rosie.github.io/) - 
- [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/) - low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface

## Audio & Speech & Music
- [Waveformer](https://waveformer.replicate.dev/) - make music from text with MusicGen and Replicate
- [Voicebox](https://voicebox.metademolab.com/) - Text-Guided Multilingual Universal Speech Generation at Scale, by MetaAI
- MERT, [Code](https://github.com/yizhilll/MERT), [Demo:HF](https://huggingface.co/spaces/m-a-p/Music-Descriptor) - an Acoustic Music Understanding Model with Large-Scale Self-supervised Training
- [MusicGen](https://ai.honu.io/papers/musicgen/), [Demo: HF](https://huggingface.co/spaces/facebook/MusicGen), [Repo](https://github.com/facebookresearch/audiocraft) -  a simple and controllable model for music generation by MetaAI
- [AI Speech Classifier](https://beta.elevenlabs.io/blog/ai-speech-classifier/), [Demo](https://beta.elevenlabs.io/ai-speech-classifier) - detect whether an audio clip was created using ElevenLabs
- [Ecoute](https://github.com/SevaSk/ecoute) - a live transcription tool that provides real-time transcripts for both the user's microphone input (You) and the user's speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI's GPT-3.5 for the user to say based on the live transcription of the conversation
- [Eleven Labs](https://beta.elevenlabs.io/) - the most realistic Text to Speech and Voice Cloning software. ElevenLabs brings the most compelling, rich and lifelike voices to creators and publishers seeking the ultimate tools for storytelling
- [SadTalker: Demo](https://huggingface.co/spaces/vinthony/SadTalker) - Stylized Audio-Driven Single Image Talking Face Animation
- [Recast](https://www.letsrecast.ai/) - turn your want-to-read articles into rich audio summaries
 - AudioGPT, [Demo: HuggingFace](https://huggingface.co/spaces/AIGC-Audio/AudioGPT), [Repo](https://github.com/AIGC-Audio/AudioGPT) - Understanding and Generating Speech, Music, Sound, and Talking Head
 - [Eleven Multilingual v1](https://beta.elevenlabs.io/blog/eleven-multilingual-v1/), [Demo](https://beta.elevenlabs.io/?ref=beta.elevenlabs.io) - generate top-quality spoken audio in any voice and style with the most advanced and multipurpose AI speech tool out there. Our deep learning model renders human intonation and inflections with unprecedented fidelity and adjusts delivery based on context
 - [Bark](https://github.com/suno-ai/bark) - a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communication like laughing, sighing and crying
- [Whisper](https://openai.com/research/whisper) - an automatic speech recognition (ASR) system, that approaches human level robustness and accuracy on English speech recognition
- [MusicLM: Generating Music From Text](https://google-research.github.io/seanet/musiclm/examples/), [Demo](https://aitestkitchen.withgoogle.com/experiments/music-lm) - a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff" by Google Research
- [Universal Speech Model (USM)](https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html) - a state-of-the-art speech AI for 100+ languages by Google Research
- [MuAViC](https://ai.facebook.com/blog/muavic-audio-visual-speech-translation-benchmark/) - a Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation by MetaAI
- [Musicfy](https://www.musicfy.lol/) - Music like you've never heard. Create and discover AI covers of your favorite songs 
- [Koe Recast](https://koe.ai/) - transform your voice using AI
- [Mubert](https://mubert.com/) - Human x AI Generative Music. For your video content, podcasts and apps

## Medical
- [Mind-Video](https://mind-video.com/) - High-quality Video Reconstruction from Brain Activity
- [Med-PaLM](https://sites.research.google/med-palm/) - a large language model (LLM) designed to provide high-quality answers to medical questions
- [PMC-LLaMA](https://github.com/chaoyi-wu/PMC-LLaMA) - the official codes for "PMC-LLaMA: Continue Training LLaMA on Medical Papers"
 
## Military
- [AIP Pillars](https://www.palantir.com/platforms/aip/) - activate LLMs and other AI on your private network, subject to full control

## Climat
- [ClimaX A foundation model for weather and climate](https://microsoft.github.io/ClimaX/) - a flexible and generalizable deep learning model for weather and climate science. [Introducing ClimaX: The first foundation model for weather and climate](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/introducing-climax-the-first-foundation-model-for-weather-and-climate/)

## Code & Math
- [GPT Engineer](https://github.com/AntonOsika/gpt-engineer) - is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt
- [CodeTF](https://github.com/salesforce/CodeTF) - a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on. It aims to facilitate easy integration of SOTA CodeLLMs into real-world applications
- [Let’s Verify Step by Step](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision#fn-1) - a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”), by OpenAI
- [🦍 Gorilla: LLM Connected with Massive APIs](https://gorilla.cs.berkeley.edu/) - a finetuned LLaMA-based model that surpasses GPT-4 on writing API calls
- [CodeT5 and CodeT5+](https://github.com/salesforce/CodeT5/tree/main) -  models can be deployed as an AI-powered coding assistant to boost the productivity of software developers, by Salesforce
- [Framer](https://www.framer.com/) - a tool that constructs a completely unique website for you based on a text prompt
- [Pico](https://picoapps.xyz/) - a tool that use GPT4 to instantly build simple, shareable web apps

## Games
- [Voyager: An Open-Ended Embodied Agent with LLMs](https://voyager.minedojo.org/) - the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention

## Other: Fin, Presentation 
- [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)
- [Gamma](https://gamma.app/) - create visually appealing presentations
