# LLMs Tools & Research Projects
The repository contains a list of ready-to-use AI Tools, Open Sources, and Research Projects \
Apart from LLMs, you can find here new AI research from other areas such as Computer Vision, etc.\
Welcome to contribute.


## Nobel Prize
**The Nobel Prize in Physics 2024** was awarded to **John J. Hopfield and Geoffrey E. Hinton** [“for foundational discoveries and inventions that enable machine learning with artificial neural networks”](https://www.nobelprize.org/prizes/physics/2024/press-release/).

**The Nobel Prize in Chemistry 2024** was awarded with one half to **David Baker** [“for computational protein design”](https://www.nobelprize.org/prizes/chemistry/2024/press-release/) and the other half jointly to **Demis Hassabis and John M. Jumper** [“for protein structure prediction”](https://www.nobelprize.org/prizes/chemistry/2024/press-release/).

Jürgen Schmidhuber's Post: [The NobelPrizeinPhysics2024 for Hopfield & Hinton rewards plagiarism and incorrect attribution in computer science](https://x.com/SchmidhuberAI/status/1844022724328394780)

## Turing Award
**TURING AWARD in 2025** was recognized **Andrew Barto and Richard Sutton** as [Pioneers of Reinforcement Learning](https://amturing.acm.org/)

## Large Language Models (LLMs) and Chatbots

### Courses
[![DeepLearning.AI](https://img.shields.io/badge/DeepLearning.AI%20|%20Andrew%20Ng-E73946?style=for-the-badge)](https://learn.deeplearning.ai/)  [![LangChain](https://img.shields.io/badge/LangChain%20Academy-1C3C3C?style=for-the-badge&logo=chainlink&logoColor=white)](https://academy.langchain.com/collections)  [![Anthropic](https://img.shields.io/badge/Anthropic%20Academy-FFD700?style=for-the-badge&logo=anthropic&logoColor=black)](https://anthropic.skilljar.com)

### Video
[Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy)\
-- [State of GPT](https://youtu.be/bZQun8Y4L2A), [[1hr Talk] Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g?si=Q30PHie5Ls_aqEFY), [How I use LLMs](https://youtu.be/EWvNQjAaOHw?si=HI1Z0GO4CDgcBVj5)\
Andrew Ng: [Opportunities in AI - 2023](https://youtu.be/5p248yoa3oE?si=LJQTHeOF-XUQB72U), [AI Dev 25: Code, learn, connect](https://www.youtube.com/playlist?list=PLkDaE6sCZn6EYD2iP6x7c3zSbJXi0_jZT)\
Sequoia Capital: [AI Ascent 2024](https://youtube.com/playlist?list=PLOhHNjZItNnOoPxOF3dmq30UxYqFuxXKn&si=peCIxzdQrATIPsg7), [AI Ascent 2025](https://youtube.com/playlist?list=PLOhHNjZItNnMEqGLRWkKjaMcdSJptkR08&si=9q9X2HHNk8FB38Op)\
[Greg Brockman | The Inside Story of ChatGPT’s Astonishing Potential](https://youtu.be/C_78DM8fG6E)\
[Sam Altman | GPT-4 Turbo | OpenAI DevDay, Opening Keynote](https://www.youtube.com/live/U9mJuUkhUzk?si=f4qPQh0_buASTp1b)\
[Gemma Developer Day Paris | Gemma 3 | Google for Developers](https://www.youtube.com/playlist?list=PLOU2XLYxmsILOkAPDwRqvbiReWbIcLC4k)\
[3Blue1Brown | Neural networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

### Reading
[Everypixel Journal | 2023: The Year of AI | Reading](https://journal.everypixel.com/2023-the-year-of-ai)\
[Stanford University | AI Index Report (since 2017)](https://aiindex.stanford.edu/report/)\
[Nathan Benaich & Alex Chalmers | State of AI Report | October, 2024](https://docs.google.com/presentation/d/1GmZmoWOa2O92BPrncRcTKa15xvQGhq7g4I4hJSNlC0M)\
[Menlo Ventures | The State of Generative AI in the Enterprise | 2024](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise)\
[Greg Kamradt | Full Stack Retrieval](https://community.fullstackretrieval.com/)\
[Prompt Engineering Guide](https://www.promptingguide.ai/)\
OpenAI: [Prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering), [Model Spec | Old](https://openai.com/index/sharing-the-latest-model-spec/), [Model Spec](https://model-spec.openai.com/2025-02-12.html)\
Anthropic: [Prompt Engineering Interactive Tutorial](https://github.com/anthropics/prompt-eng-interactive-tutorial), [Anthropic Quickstarts | Code](https://github.com/anthropics/anthropic-quickstarts)\
BentoML: [LLM Inference Handbook](https://bentoml.com/llm/)

### Visualization
[The Rise and Rise of A.I. LLMs & their associated bots like ChatGPT](https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt)\
[Opening up ChatGPT: tracking openness of instruction-tuned LLMs](https://opening-up-chatgpt.github.io/)\
[Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)\
[Can an AI make a data-driven, visual story?](https://pudding.cool/2024/07/ai/)

### Competition
[Red‑Teaming Challenge - OpenAI gpt-oss-20b](https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming) - find any previously undetected vulnerabilities and harmful behaviors — from lying and deceptive alignment to reward‑hacking exploits (Deadline: Aug, 2025)\
[AI Mathematical Olympiad - Progress Prize 2](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2) - solve national-level math challenges using artificial intelligence models (Deadline: Mar, 2025)\
[Google - Unlock Global Communication with Gemma](https://www.kaggle.com/competitions/gemma-language-tuning) - create Gemma model variants for a specific language or unique cultural aspect (Deadline: Jan, 2024)\
[Google - Gemini Long Context](https://www.kaggle.com/competitions/gemini-long-context) - demonstrate interesting use cases for Gemini's long context window (Deadline: Dec, 2024)\
[Gemini API Developer Competition](https://ai.google.dev/competition) - build incredible apps with the Gemini API, $1 million in cash prizes (Deadline: Sep, 2024)

<!---
||Google|OpenAI|Meta|EleutherAI|Stability AI|Anthropic|
:-:|:-:|:-:|:-:|:-:|:-:|:-:
2023|[PaLM-2](https://ai.google/discover/palm2)<br>[Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/)|[GPT-4](https://openai.com/product/gpt-4)|LLAMA2<br>[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)|[Pythia](https://github.com/EleutherAI/pythia)|[Stable Vicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)<br>[StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)|[Claude2](claude.ai)<br>[Claude](https://www.anthropic.com/product)|
2022|[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)<br>[GLaM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)|[ChatGPT](https://openai.com/blog/chatgpt)|Galactica|GPT-NeoX<br>GPT Neo||RL-CAI|
2021|[LaMDA](https://blog.google/technology/ai/lamda/)|||GPT-J||||
--->

### Models
||2021-22|2023|2024|2025
:-:|:-:|:-:|:-:|:-:
[Google](https://blog.google/),<br>[DeepMind](https://deepmind.google/)|[LaMDA](https://blog.google/technology/ai/lamda/), [GLaM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)<br>[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), [Chinchilla](https://deepmind.google/discover/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training/)|[Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/), [PaLM-2](https://ai.google/discover/palm2), [Gemini](https://deepmind.google/technologies/gemini/#introduction)|[Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/), [Gemini 1.5 Flash](https://blog.google/technology/developers/gemini-gemma-developer-updates-may-2024/),<br>[Gemma](https://blog.google/technology/developers/gemma-open-models/), [Gemma 2](https://blog.google/technology/developers/google-gemma-2/), [Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024)|[Gemini 2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking),<br>[Gemma 3](https://blog.google/technology/developers/gemma-3), [Gemini 2.5](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/),<br>[Gemini 2.5 Flash](https://developers.googleblog.com/en/start-building-with-gemini-25-flash),<br>[Gemma 3n](https://developers.googleblog.com/en/introducing-gemma-3n), [Gemini Diffusion](https://deepmind.google/models/gemini-diffusion/), [Gemma 3 270M](https://developers.googleblog.com/en/introducing-gemma-3-270m)|
[OpenAI](https://openai.com/)|[ChatGPT](https://openai.com/blog/chatgpt)|[GPT-4](https://openai.com/product/gpt-4), [GPT-4 Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)|[GPT-4o](https://openai.com/index/hello-gpt-4o/), [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/), [CriticGPT](https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/),<br>[o1-preview](https://openai.com/index/introducing-openai-o1-preview/), [o1-mini](https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/), [o1](https://openai.com/o1/)|[o3-mini](https://openai.com/index/openai-o3-mini), [Deep Research](https://openai.com/index/introducing-deep-research),<br>[GPT-4.5](https://openai.com/index/introducing-gpt-4-5), [GPT-4.1](https://openai.com/index/gpt-4-1),<br>[o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini), [gpt-oss](https://openai.com/index/introducing-gpt-oss/), [GPT-5](https://openai.com/index/introducing-gpt-5/)|
[MetaAI](https://ai.meta.com/)|Galactica|[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), [LLaMA2: HF](https://huggingface.co/blog/llama2),<br>[Purple Llama](https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/)|[Llama 3](https://ai.meta.com/blog/meta-llama-3/), [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/), [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/),<br>[quantized Llama](https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/), [Llama 3.3 70B](https://www.llama.com/)|[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence)|
[Mistral AI](https://mistral.ai/)||[Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/),<br>[Mixtral of experts](https://mistral.ai/news/mixtral-of-experts)|[Mistral Large](https://mistral.ai/news/mistral-large/), [Mistral Large 2](https://mistral.ai/news/mistral-large-2407/),<br>[Mistral NeMo](https://mistral.ai/news/mistral-nemo/), [Pixtral 12B](https://mistral.ai/news/pixtral-12b/),<br>[Pixtral Large](https://mistral.ai/news/pixtral-large/),<br>[Ministral 3B and Ministral 8B](https://mistral.ai/news/ministraux/)|[Mistral Small 3](https://mistral.ai/news/mistral-small-3/),<br>[Mistral Small 3.1](https://mistral.ai/news/mistral-small-3-1),<br>[Mistral Medium 3](https://mistral.ai/news/mistral-medium-3), [Magistral](https://mistral.ai/news/magistral)|
[Anthropic](https://www.anthropic.com/)|RL-CAI|[Claude](https://www.anthropic.com/product), [Claude2](https://www.anthropic.com/index/claude-2), [Claude2.1](https://www.anthropic.com/index/claude-2-1)|[Claude 3](https://www.anthropic.com/news/claude-3-family),<br>[Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)|[Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet),<br>[Claude for Education](https://www.anthropic.com/news/introducing-claude-for-education),<br>[Claude 4](https://www.anthropic.com/news/claude-4), [Claude Opus 4.1](https://www.anthropic.com/news/claude-opus-4-1), [Claude Sonnet 4.5](https://www.anthropic.com/news/claude-sonnet-4-5)|
Microsoft||[phi-1](https://huggingface.co/microsoft/phi-1), [phi-1.5](https://huggingface.co/microsoft/phi-1_5), [phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)|[phi-3](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/), [phi-3.5](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280), [phi-4](https://huggingface.co/microsoft/phi-4)|[phi-4-multimodal](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family), [Phi-4-reasoning](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai)|
[Stability AI](https://stability.ai/)||[Stable Vicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot), [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models),<br>[Stable LM 3B](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices), [Stable Beluga](https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models), [Stable Chat](https://stability.ai/blog/stable-chat-research-defcon-ai-village),<br>[Stable LM Zephyr 3B](https://stability.ai/news/stablelm-zephyr-3b-stability-llm)|[Stable LM 2 1.6B](https://stability.ai/news/introducing-stable-lm-2), [Stable LM 2 12B](https://stability.ai/news/introducing-stable-lm-2-12b)|
[Inflection AI](https://inflection.ai/)||[Inflection-2](https://inflection.ai/inflection-2)|[Inflection-2.5](https://inflection.ai/inflection-2-5)|
[TII](https://www.tii.ae/)||[Falcon](https://huggingface.co/blog/falcon)|[Falcon Mamba 7B](https://huggingface.co/tiiuae/falcon-mamba-7b-instruct),<br>[Falcon 3](https://huggingface.co/blog/falcon3)|[Falcon-Edge](https://huggingface.co/blog/tiiuae/falcon-edge), [Falcon-H1](https://huggingface.co/blog/tiiuae/falcon-h1),<br>[Falcon-Arabic](https://huggingface.co/blog/tiiuae/falcon-arabic)|
[Cohere](https://cohere.com/)||[Aya](https://cohere.com/blog/aya-multilingual)|[Command R+](https://cohere.com/blog/command-r-plus-microsoft-azure), [Rerank 3](https://txt.cohere.com/rerank-3/),<br>[Aya Expanse](https://cohere.com/blog/aya-expanse-connecting-our-world)|[Aya Vision](https://cohere.com/blog/aya-vision), [Command A](https://cohere.com/blog/command-a),<br>[Command A Reasoning](https://cohere.com/blog/command-a-reasoning)|
[xAI](https://x.ai/blog)|||[Grok-1](https://x.ai/blog/grok-os), [Grok-1.5](https://x.ai/blog/grok-1.5), [Grok-2](https://x.ai/blog/grok-2)|[Grok-3](https://x.ai/news/grok-3), [Grok-4](https://x.ai/news/grok-4)|
NVIDIA|||[Nemotron-4 340B](https://research.nvidia.com/publication/2024-06_nemotron-4-340b),<br>[Minitron-4B-Base](https://huggingface.co/nvidia/Minitron-4B-Base), [NVLM 1.0](https://research.nvidia.com/labs/adlr/NVLM-1/),<br>[Llama-3.1-Nemotron-70B-Instruct](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF)|[NVIDIA Llama Nemotron](https://developer.nvidia.com/blog/build-enterprise-ai-agents-with-advanced-open-nvidia-llama-nemotron-reasoning-models)|
[AI2](https://allenai.org/blog)|||[Molmo](https://allenai.org/blog/molmo), [Tulu3](https://allenai.org/blog/tulu-3), [OLMo 2](https://allenai.org/blog/olmo2)|[Tülu 3 405B](https://allenai.org/blog/tulu-3-405B), [OLMo 2 32B](https://allenai.org/blog/olmo2-32B)|
[AI21Lab](https://www.ai21.com/)|||[Jamba](https://www.ai21.com/blog/announcing-jamba), [Jamba 1.5](https://www.ai21.com/blog/announcing-jamba-model-family)|[Jamba 1.6](https://www.ai21.com/blog/introducing-jamba-1-6)|
[Abacus.AI](https://abacus.ai/)||[Giraffe](https://blog.abacus.ai/blog/2023/08/22/giraffe-long-context-llms/)|[Smaug-72B-v0.1](https://huggingface.co/abacusai/Smaug-72B-v0.1)|
Alibaba Cloud|||[Qwen](https://qwenlm.github.io/blog/qwen/), [Qwen2](https://qwenlm.github.io/blog/qwen2/), [Qwen2.5](https://qwenlm.github.io/blog/qwen2.5-llm)|[Qwen2.5-Max](https://qwenlm.github.io/blog/qwen2.5-max/), [QwQ-32B](https://qwenlm.github.io/blog/qwq-32b),<br>[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/), [Qwen3](https://qwenlm.github.io/blog/qwen3),<br>[Qwen3-Next](https://qwen.ai/blog?id=e34c4305036ce60d55a0791b170337c2b70ae51d&from=home.latest-research-list)|
AWS||[Titan](https://aws.amazon.com/bedrock/amazon-models/titan/)|[Nova](https://aws.amazon.com/ai/generative-ai/nova/)|[Nova Act](https://labs.amazon.science/blog/nova-act), [Nova Premier](https://aws.amazon.com/blogs/aws/amazon-nova-premier-our-most-capable-model-for-complex-tasks-and-teacher-for-model-distillation)|
[DeepSeek](https://api-docs.deepseek.com/)|||[DeepSeek-V2.5](https://api-docs.deepseek.com/news/news1210), [DeepSeek-V3](https://api-docs.deepseek.com/news/news1226),<br>[DeepSeek-R1-Lite-Preview](https://api-docs.deepseek.com/news/news1120)|[DeepSeek-R1](https://api-docs.deepseek.com/news/news250120), [DeepSeek-V3.1](https://api-docs.deepseek.com/news/news250821), [DeepSeek-V3.2-Exp](https://api-docs.deepseek.com/news/news250929)|

### Open Source Models
|Model|Company|Date|Notes|
:-|:-:|:-:|:-:
|[gpt-oss](https://openai.com/index/introducing-gpt-oss/)|OpenAI|2025-08-05|20b and 120b|
|[MiniMax Family](https://github.com/MiniMax-AI/MiniMax-01)|MiniMax|2025-01-15|language and visual|
|[OLMo 2](https://allenai.org/blog/olmo2)|AI2|2024-11-26||
|[Qwen2.5 Family](https://qwenlm.github.io/blog/qwen2.5/)|Alibaba Cloud|2024-09-19|some versions|
|[phi-3](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)|Microsoft|2023-05-21|
|[Qwen2 Family](https://qwenlm.github.io/blog/qwen2/)|Alibaba Cloud|2024-06-07|some versions|
|[Llama Family](https://llama.meta.com/)|MetaAI|||
|[DBRX](https://www.databricks.com/company/newsroom/press-releases/databricks-launches-dbrx-new-standard-efficient-open-source-models)|Databricks|2024-03-27|a general purpose LLM|
|[Gemma](https://blog.google/technology/developers/gemma-open-models/)|Google|2024-02-21||
|[phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)|Microsoft|2023-12-12||
|[Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/)|Mistral|2023-09-27|Apache 2.0|

- [Granite 4.0](https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models), [Granite 3.2](https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision), [Granite 3.0](https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models), [Granite](https://www.ibm.com/granite) - a family of open, performant, and trusted AI models, tailored for business and optimized to scale your AI applications, by IBM
- [GLM-4.6](https://z.ai/blog/glm-4.6), [GLM-4.5](https://z.ai/blog/glm-4.5) - Advanced Agentic, Reasoning and Coding Capabilities
- [Cogito v1 Preview](https://www.deepcogito.com/research/cogito-v1-preview), [Cogito v2 Preview](https://www.deepcogito.com/research/cogito-v2-preview) - from inference-time search to self-improvement (4 hybrid reasoning models)
- [Kimi k1.5](https://github.com/MoonshotAI/Kimi-k1.5) - Scaling Reinforcement Learning with LLMs, [Kimi K2](https://moonshotai.github.io/Kimi-K2/) - Open Agentic Intelligence
- [MiniMax-M1](https://huggingface.co/MiniMaxAI/MiniMax-M1-40k) - the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism
- [BAGEL](https://bagel-ai.org/) - the open-source Unified Multimodal Model you can fine-tune, distill and deploy anywhere, offering comparable functionality to proprietary systems like GPT-4o and Gemini 2.0 in an open form
- [Open-R1](https://huggingface.co/blog/open-r1), [updates](https://huggingface.co/blog/open-r1/update-3) - a fully open reproduction of DeepSeek-R1
- [Mercury](https://www.inceptionlabs.ai/news) - diffusion LLM that are up to 10x faster and cheaper than current LLMs, pushing the frontier of intelligence and speed for LMs, by Inception
- [SmolLM](https://huggingface.co/blog/smollm), [SmolVLM2](https://huggingface.co/blog/smolvlm2) - a family of state-of-the-art small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset, by Hugging Face
- [LLaDA](https://ml-gsai.github.io/LLaDA-demo) - Large Language Diffusion with mAsking - a diffusion model with an unprecedented 8B scale, trained entirely from scratch, rivaling LLaMA3 8B in performance
- [R1 1776](https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776) - a version of the DeepSeek-R1 model that has been post-trained to provide unbiased, accurate, and factual information
- [s1](https://github.com/simplescaling/s1) - minimal recipe for test-time scaling and strong reasoning performance matching o1-preview with just 1,000 examples & budget forcing
- [Sky-T1](https://novasky-ai.github.io/posts/sky-t1/) - reasoning model that performs on par with o1-preview on popular reasoning and coding benchmarks
- [Transformer²](https://sakana.ai/transformer-squared/) - a ML system that dynamically adjusts its weights for various tasks, by Sakana
- [SmallThinker-3B-preview](https://huggingface.co/PowerInfer/SmallThinker-3B-Preview) - a 3 billion parameter o1-like language model designed to excel at reasoning tasks (fine-tuned from the Qwen2.5-3b-Instruct model)
- [voyage-3 & voyage-3-lite](https://blog.voyageai.com/2024/09/18/voyage-3/) - a new generation of small yet mighty general-purpose embedding models
- [Tencent-Hunyuan-Large](https://github.com/Tencent/Tencent-Hunyuan-Large) - the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters
- [Paperguide](https://paperguide.ai/) - AI Research Assistant, Reference Manager and Writing Assistant that help you understand papers, manage references, annotate/take notes, and supercharge your writing
- Gemma Scope [Demo](https://www.neuronpedia.org/gemma-scope) - a beginner-friendly introduction to interpretability that explores an AI model called Gemma 2 2B. It also contains interesting and relevant content even for those already familiar with the topic
- [Hermes 3](https://nousresearch.com/hermes3/) - the latest version in our Hermes series, available in 3 sizes, 8, 70, and 405B parameters
- [SearchGPT](https://openai.com/index/searchgpt-prototype/) - a temporary prototype of new AI search features that give you fast and timely answers with clear and relevant sources
- [InternLM 2.5](https://github.com/InternLM/InternLM) - outstanding reasoning capability, 1M context window, stronger tool use
- [FILM](https://github.com/microsoft/FILM) - repo can help you to reproduce the results of FILM-7B, a 32K-context LLM that overcomes the lost-in-the-middle problem. FILM-7B is trained from Mistral-7B-Instruct-v0.2 by applying Information-Intensie (In2) Training, by Microsoft
- [gpt2-chatbots (aka GPT-4o)](https://rentry.org/GPT2)
- [Snowflake Arctic](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/) - an enterprise-focused LLM designed to provide cost-effective training and openness
- [Reka Core](https://www.reka.ai/news/reka-core-our-frontier-class-multimodal-language-model) - Multimodal LLM
- [ChatFlow](https://chatflow.no/) - a no-code platform that lets you set up an OpenAI-powered chatbot for your website
- [Ferret](https://github.com/apple/ml-ferret) - An End-to-End MLLM that Accept Any-Form Referring and Ground Anything in Response, by Apple
- [NotebookLM](https://notebooklm.google/) - a powerful new interface that lets you shift effortlessly from reading to asking questions to writing, with an AI thought partner helping you at every turn
- [LLM360](https://www.llm360.ai/) - enables community-owned AGI through open-source large model research and development (K2-65B, CrystalCoder-7B, Amber-7B)
- [Mirasol](https://research.google/blog/scaling-multimodal-understanding-to-long-videos/) - a multimodal model for learning across audio, video, & text that decouples the modeling into separate autoregressive models to process the inputs according to the characteristics of their modalities, for state-of-the-art performance, by Google
- [UniIR](https://tiger-ai-lab.github.io/UniIR/) - Universal Multimodal Information Retrievers, framework to learn a single retriever to accomplish (possibly) any retrieval task
- [Tulu-2-DPO model](https://www.interconnects.ai/p/rlhf-progress-scaling-dpo-to-70b) - RLHF method DPO scales to 70B parameters, clearly compare PEFT fine-tuning to full-parameter fine-tuning
- [Phind](https://www.phind.com/blog/phind-model-beats-gpt4-fast), [Phind-70B](https://www.phind.com/blog/introducing-phind-70b) - model that matches and exceeds GPT-4's coding abilities while running 5x faster
- [Vicuna-13B](https://lmsys.org/blog/2023-03-30-vicuna/) - an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT, by [Vicuna Team](https://lmsys.org/about/)
- [Alpaca 7B](https://crfm.stanford.edu/2023/03/13/alpaca.html) - a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations
- [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) -  a chatbot trained by fine-tuning Meta’s LLaMA on dialogue data gathered from the web, by [Berkeley-BAIR](https://bair.berkeley.edu/)
- [FacTool](https://github.com/GAIR-NLP/factool) - a tool augmented framework for detecting factual errors of texts generated by LLMs. Factool now supports 4 tasks: knowledge-based QA, code generation, mathematical reasoning, scientific literature review
- [Nougat](https://facebookresearch.github.io/nougat/) - Neural Optical Understanding for Academic Documents, a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents, by MetaAI
- [TextFX](https://textfx.withgoogle.com/) - AI-powered tools for rappers, writers and wordsmiths
- [Prompt2Model](https://github.com/neulab/prompt2model) - a system that takes a natural language task description (like the prompts used for LLMs such as ChatGPT) to train a small special-purpose model that is conducive for deployment
- [ToolBench](https://github.com/OpenBMB/ToolBench) - open-source, large-scale, high-quality instruction tuning SFT data to facilitate the construction of powerful LLMs with general tool-use capability
- [Platypus](https://platypus-llm.github.io/) - a family of fine-tuned and merged LLMs that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work
- [OpenFlamingo V2](https://laion.ai/blog/open-flamingo-v2/) - an open-source effort to replicate DeepMind's Flamingo models
- [MetaGPT](https://github.com/geekan/metagpt) - a framework involving LLM-based multi-agents that encodes human standardized operating procedures (SOPs) to extend complex problem-solving capabilities that mimic efficient human workflows
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/)
- [FlashAttention](https://princeton-nlp.github.io/flash-atttention-2/) - an algorithm to speed up attention and reduce its memory footprint—without any approximation
- [Quivr](https://github.com/StanGirard/quivr) - utilizes the power of Generative AI to store and retrieve unstructured information
- [LongLLaMA](https://github.com/CStanKonrad/long_llama) - a LLM capable of handling long contexts of 256k tokens or even more
- [OpenLLaMA](https://github.com/openlm-research/open_llama) - open source reproduction of MetaAI’s LLaMA
- [BuboGPT](https://bubo-gpt.github.io/) - an advanced LLM that incorporates multi-modal inputs including text, image and audio, with a unique ability to ground its responses to visual objects
- [LAION](https://laion.ai/) - Large-scale Artificial Intelligence Open Network
- [GPT4All](https://gpt4all.io/index.html), [Code](https://github.com/nomic-ai/gpt4all) - an open-source assistant-style LLM that run locally on your CPU
- [SdkVercelAI](https://play.vercel.ai/) - you can input a prompt, pick different LLMS, and compare two side by side
- [Open Assistant](https://open-assistant.io/) - a completely open-source ChatGPT alternative
- [Baize](https://github.com/project-baize/baize-chatbot) - an open-source chat model trained with LoRA. It uses 100k dialogs generated by letting ChatGPT chat with itself
- [Chameleon](https://chameleon-llm.github.io/) - a compositional reasoning framework designed to enhance LLMs and overcome their inherent limitations, such as outdated information and lack of precise reasoning
- [Bloom](https://huggingface.co/bigscience/bloom) - an autoregressive LLM, trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources
- [Pythia](https://github.com/EleutherAI/pythia) - the hub for [EleutherAI's](https://www.eleuther.ai/) work on interpretability and learning dynamics

### Chats & Assistants
|Chat|Company|Notes|
:-|:-:|:-|
|[Andrew Ng](https://www.deeplearning.ai/avatar/)|DeeplearningAI|Chat with Andrew Ng|
|[Playground](https://playground.allenai.org/)|THe Allen Institute for AI, AI2||
|[Stable Assistant](https://stability.ai/stable-assistant)|Stability AI|latest text and image generation technology featuring Stable Diffusion 3,<br>Stable Video, Stable Image Services and Stable LM 2 12B|
|[Moshi](https://www.moshi.chat/)|Kyutai|engaging conversations limited to 5 minutes, thinks and speaks at the same time|
|[MetaAI](https://www.meta.ai)|MetaAI||
|[character.ai](https://character.ai/)|Character.AI|talk with fictional AI characters|
|[POE](https://poe.com/)|Quora|talk to ChatGPT, GPT-4, Claude 3 Opus, DALLE 3, and millions of others|
|[Hume](https://demo.hume.ai/)|Hume|empathic AI voice chat|
|[Pi](https://pi.ai/)|Inflection AI||
|[Gemini](https://gemini.google.com/app)|Google||
|[ChatRTX](https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai)|Nvidia|runs locally on your PC|
|[Le Chat](https://mistral.ai/news/le-chat-mistral/)|Mistral AI||
|[Copilot](https://copilot.microsoft.com/)|Microsoft||
|[Perplexity](https://www.perplexity.ai/)|PerplexityAI|
|[ChatGPT](https://chat.openai.com/)|OpenAI||

- [AI Chat](https://aichat.fm/) - get ChatGPT, Gemini, Claude, Grok & Husky AI - without jumping between apps
- [SciSummary](https://scisummary.com/) - use AI to summarize scientific articles and research papers
- [PdfGPT](https://pdfgpt.io/) - summarize any PDF; get instant, accurate answers from long research papers, legal documents, and manuals
- [ChatPDF](https://www.chatpdf.com/) - chat with any PDF
- [Dmwithme](https://dmwithme.com) - virtual companion with realistic emotions that roleplays as ai friend

### Offline-Mode
- [Google AI Edge Gallery](https://github.com/google-ai-edge/gallery) - an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android (available now) and iOS (coming soon) devices
- [msty](https://msty.app/) - the easiest way to use local and online AI models
- [aider](https://github.com/paul-gauthier/aider) - AI pair programming in your terminal
- [Open Interpreter](https://github.com/KillianLucas/open-interpreter) - an open-source, locally running implementation of OpenAI's Code Interpreter
- [ollama](https://ollama.com/) - get up and running with Llama 3, Mistral, Gemma, and other LLMs
- [Dalai](https://cocktailpeanut.github.io/dalai) - run LLaMA and Alpaca on your computer
- [LLaMAChat](https://llamachat.app/) - allows you to chat with LLaMa, Alpaca and GPT4All models all running locally on your CPU
- [OpenLLM](https://github.com/bentoml/OpenLLM) - an open-source platform designed to facilitate the deployment and operation of LLMs in real-world applications
- [LM Studio](https://lmstudio.ai/) - an easy way to run open-source LLMs locally
- [Jan](https://jan.ai/) - open-source ChatGPT alternative that runs 100% offline on your computer
- [Pinokio](https://pinokio.computer/) - a browser that lets you install, run, and programmatically control ANY application, automatically

### Large Visual Language Models (LVLMs)
- [QVQ-Max](https://qwenlm.github.io/blog/qvq-max-preview) - visual reasoning model that can not only “understand” the content in images and videos but also analyze and reason with this information to provide solutions
- [PaliGemma](https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/), [PaliGemma 2](https://developers.googleblog.com/en/introducing-paligemma-2-powerful-vision-language-models-simple-fine-tuning/), [PaliGemma 2 Mix](https://developers.googleblog.com/en/introducing-paligemma-2-mix/) - fine-tuned on a mix of vision language tasks, including OCR, long and short captioning and more, by Google
- [Eagle Family](https://github.com/NVlabs/EAGLE) - Exploring Model Designs, Data Recipes and Training Strategies for Frontier-Class Multimodal LLMs, by NVIDIA
- [Qwen-VL](https://github.com/QwenLM/Qwen-VL), [Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/), [Qwen2.5-VL](https://qwenlm.github.io/blog/qwen2.5-vl/) - a significant leap from the previous Qwen2-VL: Understand things visually; Being agentic; Understanding long videos and capturing events; Capable of visual localization in different formats; Generating structured outputs
- [Janus](https://github.com/deepseek-ai/Janus) - Unified Multimodal Understanding and Generation Models, by DeepSeek
- [Moondream 1.9B](https://moondream.ai/blog/introducing-a-new-moondream-1-9b-and-gpu-support) - a tiny open-source vision AI that brings powerful image understanding to your applications and runs everywhere
- [Idefics2](https://huggingface.co/blog/idefics2) - it can answer questions about images, describe visual content, create stories grounded in multiple images, extract information from documents, and perform basic arithmetic operations
- [Grok-1.5 Vision](https://x.ai/blog/grok-1.5v) - can process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs, by xAI
- [CogVLM & CogAgent](https://github.com/THUDM/CogVLM) - an 18 billion parameter visual language model specializing in GUI understanding and navigation; supports high-resolution inputs (1120x1120) and shows abilities in tasks such as visual Q&A, visual grounding, and GUI Agent
- [AnyText](https://github.com/tyxsspa/AnyText) - Multilingual Visual Text Generation And Editing
- [AnomalyGPT](https://github.com/CASIA-IVA-Lab/AnomalyGPT) - the LVLM based Industrial Anomaly Detection (IAD) method that can detect anomalies in industrial images without the need for manually specified thresholds
- [IDEFICS](https://huggingface.co/blog/idefics) - an open-access VLM based on Flamingo. The model accepts arbitrary sequences of image and text inputs and produces text outputs, aiming to bring transparency to AI systems and serve as a foundation for open research in multimodal AI systems
- [Prismer](https://shikun.io/projects/prismer) - a data- and parameter-efficient VLM that leverages an ensemble of diverse, pre-trained domain experts
- [MiniGPT-4](https://minigpt-4.github.io/) - upload an image, and then use chat to identify what's in the picture and learn more about it
- [MultiModal-GPT](https://github.com/open-mmlab/Multimodal-GPT) - a vision and language model for multi-round dialogue with humans; the model is fine-tuned from OpenFlamingo, with LoRA added in the cross-attention and self-attention parts of the language model
- [MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA) - Mixture-of-Experts for Large Vision-Language Models
- [LLaVA](https://llava-vl.github.io/) - a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding
- [TaskMatrix](https://github.com/microsoft/visual-chatgpt) - connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting

### Evaluation
- [Humanity's Last Exam](https://agi.safe.ai/) - a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage
- [LMEval](https://opensource.googleblog.com/2025/05/announcing-lmeval-an-open-ource-framework-cross-model-evaluation.html) - a large model evaluation framework, to help others accurately and efficiently compare how models from various providers perform across benchmark datasets, by Google
- [SuperGPQA](https://supergpqa.github.io/) - a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of LLMs across 285 graduate-level disciplines
- [FACTS Grounding](https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models) - a new benchmark for evaluating the factuality of LLMs, by DeepMind
- [MLE-bench](https://github.com/openai/mle-bench/) - a benchmark for measuring how well AI agents perform at machine learning engineering, by OpenAI
- [JailbreakBench](https://jailbreakbench.github.io/) - Repository of jailbreak artifacts, Standardized evaluation framework, Leaderboard, Dataset
- [SWE-bench Verified](https://openai.com/index/introducing-swe-bench-verified/) - a benchmark for evaluating LLMs’ abilities to solve real-world software issues sourced from GitHub, by OpenAI
- [SWE-bench](https://www.swebench.com/) - Can Language Models Resolve Real-world Github Issues?
- [promptbench](https://github.com/microsoft/promptbench) - a Unified Library for Evaluating and Understanding LLM
- [Vibe-Eval](https://www.reka.ai/news/vibe-eval) - evaluation suite for measuring progress of multimodal language models, by Reka
- [FACET](https://ai.meta.com/blog/dinov2-facet-computer-vision-fairness-evaluation) - FAirness in Computer Vision EvaluaTion - a new comprehensive benchmark for evaluating the fairness of computer vision models across classification, detection, instance segmentation, and visual grounding tasks
- [Arthur Bench](https://www.arthur.ai/blog/introducing-arthur-bench) - an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models
- [AgentBench](https://github.com/THUDM/AgentBench) - the first benchmark designed to evaluate LLM-as-Agent across a diverse spectrum of different environments
- [L-Eval](https://github.com/OpenLMLab/LEval) - a comprehensive long-context language models evaluation suite with 18 long document tasks across multiple domains that require reasoning over long texts, including summarization, question answering, in-context learning with long CoT examples, topic retrieval, and paper writing assistance
- [OpenICL](https://github.com/Shark-NLP/OpenICL) - an open-source toolkit for in-context learning and LLM evaluation; supports various state-of-the-art retrieval and inference methods, tasks, and zero-/few-shot evaluation of LLMs
- [OpenAGI](https://github.com/agiresearch/OpenAGI) - an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models

### Leaderboards
- [Embedding Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - compares 100+ text and image embedding models across 1000+ languages
- [Game Arena](https://www.kaggle.com/game-arena) - a new benchmarking platform where AI models and agents compete head-to-head in a variety of strategic games to help chart new frontiers for trustworthy AI evaluation, by Kaggle
- [OpenRouter](https://openrouter.ai/rankings) - a unified interface for LLMs
- [MathArena](https://matharena.ai/) - a platform for evaluation of LLMs on the latest math competitions and olympiads
- [EU AI Act Compliance Leaderboard](https://huggingface.co/spaces/latticeflow/compl-ai-board) - the high-level regulatory requirements of the EU AI Act as concrete technical requirements
- [AgentBoard](https://hkust-nlp.github.io/agentboard/) - a benchmark designed for multi-turn LLM agents, complemented by an analytical evaluation board for detailed model assessment beyond final success rates
- [LLM Hallucination Index](https://www.rungalileo.io/hallucinationindex) - A Ranking & Evaluation Framework For LLM Hallucinations
- [Artificial Analysis](https://artificialanalysis.ai/text-to-image) - Text to Image AI Model & Provider Leaderboard across quality, generation time, and price
- [SEAL Leaderboards](https://scale.com/leaderboard) - Safety, Evaluations and Alignment Lab: (i) generate code, (ii) work on Spanish-language inputs and outputs, (iii) follow detailed instructions, and (iv) solve fifth-grade math problems, by Scale AI
- [HELM](https://crfm.stanford.edu/helm) - Holistic Evaluation of Language Models projec - leaderboards with many scenarios, metrics, and models with support for multimodality and model-graded evaluation, by Stanford
- [vals.ai](https://www.vals.ai/) - an independent model testing service, developed benchmarks that rank LLM performance of tasks associated with income taxes, corporate finance, and contract law; it also maintains a pre-existing legal benchmark, by Vals AI
- [TrustLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/leaderboard.html) - a comprehensive study of Trustworthiness in LLMs
- [LMSYS Chatbot Arena](https://chat.lmsys.org/) - an open platform to evaluate LLMs by human preference in the real-world
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) - evaluate models on 6 key benchmarks using the [Eleuther AI Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness), a unified framework to test generative language models on a large number of different evaluation tasks
- [LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) - a benchmark the performance (latency, throughput, memory & energy) of LLMs with different hardwares, backends and optimizations using [Optimum-Benhcmark](https://github.com/huggingface/optimum-benchmark)
- [Hallucinations Leaderboard](https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard) - evaluates the propensity for hallucination in LLMs across a diverse array of tasks, including Closed-book Open-domain QA, Summarization, Reading Comprehension, Instruction Following, Fact-Checking, and Hallucination Detection
- [NPHardEval leaderboard](https://huggingface.co/spaces/NPHardEval/NPHardEval-leaderboard) - a benchmark for assessing the reasoning abilities of LLMs through the lens of computational complexity classes
- [LLM Safety Leaderboard](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard) - evaluation for LLM safety and help researchers and practitioners better understand the capabilities, limitations, and potential risks of LLMs
- [The Open Medical-LLM Leaderboard](https://huggingface.co/blog/leaderboard-medicalllm) - aims to track, rank and evaluate the performance of LLMs on medical question answering tasks
- [TheFastest.AI](https://thefastest.ai/) - site that provides reliable measurements for the performance of popular models
- [GAIA Leaderboard](https://huggingface.co/spaces/gaia-benchmark/leaderboard) - evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc)

## Datasets
- [Instruction tuning datasets](https://github.com/zhilizju/Awesome-instruction-tuning) - open-source instruction tuning datasets, models, papers, repositories
- [InfiMM-WebMath-40B Dataset](https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B) - large-scale, open-source multimodal dataset specifically designed for mathematical reasoning tasks
- [MMMLU](https://huggingface.co/datasets/openai/MMMLU) - Multilingual Massive Multitask Language Understanding
- [Natural Questions](https://ai.google.com/research/NaturalQuestions) - contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question

## Libraries
- [aisuite](https://github.com/andrewyng/aisuite) ![GitHub Repo stars](https://img.shields.io/github/stars/andrewyng/aisuite?style=plastic) - simple, unified interface to multiple Generative AI providers, by Andrew Ng Team
- [LangChain](https://github.com/hwchase17/langchain) ![GitHub Repo stars](https://img.shields.io/github/stars/langchain-ai/langchain?style=plastic&logo=langchain), [Tutorials](https://python.langchain.com/docs/tutorials/) - a framework for developing applications powered by language models
- [Open Agent Platform](https://github.com/langchain-ai/open-agent-platform/) ![GitHub Repo stars](https://img.shields.io/github/stars/langchain-ai/open-agent-platform?style=plastic) - a no-code agent building platform, by LangChain
- [Gemini Fullstack LangGraph](https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart) ![GitHub Repo stars](https://img.shields.io/github/stars/google-gemini/gemini-fullstack-langgraph-quickstart?style=plastic) - application serves as an example of building research-augmented conversational AI using LangGraph and Google's Gemini models
- [LlamaIndex](https://github.com/jerryjliu/llama_index), [docs](https://gpt-index.readthedocs.io/en/latest/) - a “data framework” to help you build LLM apps
- [LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory) - an open-source toolkit for pre-training, fine-tuning and deployment of LLMs and mutlimodal LLMs
- [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) - a lightweight adaption method for fine-tuning Instruction-following and Multi-modal LLaMA models
- [streaming-llm](https://github.com/mit-han-lab/streaming-llm) - Efficient Streaming Language Models with Attention Sinks
- [llamafile](https://github.com/Mozilla-Ocho/llamafile) - run LLMs with a single file
- [outlines](https://github.com/outlines-dev/outlines), [docs](https://outlines-dev.github.io/outlines/) - a library to write reliable programs for interactions with generative models: language models, diffusers, multimodal models, classifiers, etc
- [OneLLM](https://onellm.csuhan.com/) - One Framework to Align All Modalities with Language
- [guidance](https://github.com/guidance-ai/guidance) - interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text
- [nanoGPT](https://github.com/karpathy/nanoGPT) - the simplest, fastest repository for training/finetuning medium-sized GPTs
- [TorchScale](https://github.com/microsoft/torchscale) - a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively
- [InvokeAI](https://invoke-ai.github.io/InvokeAI/) - an implementation of Stable Diffusion, the open source text-to-image and image-to-image generator
- [ComfyUI](https://github.com/comfyanonymous/ComfyUI) - a powerful and modular Stable Diffusion GUI and backend. This UI will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface
- [StableSwarmUI](https://github.com/Stability-AI/StableSwarmUI) - Modular Stable Diffusion Web-User-Interface, with an emphasis on making powertools easily accessible, high performance, and extensibility
- [Wanda](https://github.com/locuslab/wanda) - Pruning LLMs by Weights and Activation: removes weights on a per-output basis, by the product of weight magnitudes and input activation norms
- [LOMO: LOw-Memory Optimization](https://github.com/OpenLMLab/LOMO) - a new optimizer, which fuses the gradient computation and the parameter update in one step to reduce memory usage
- [LMFlow](https://github.com/OptimalScale/LMFlow) - an extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community
- [Heron](https://github.com/turingmotors/heron) - a library that seamlessly integrates multiple Vision and Language models, as well as Video and Language models. Additionally, we provide pretrained weights trained on various datasets
- [Curated Transformers](https://github.com/explosion/curated-transformers) - a transformer library for PyTorch. It provides state-of-the-art models that are composed from a set of reusable components, by Explosion
- [spacy-llm](https://github.com/explosion/spacy-llm) - integrates LLMs into spaCy, featuring a modular system for fast prototyping and prompting, and turning unstructured responses into robust outputs for various NLP tasks, no training data required, by Explosion
- [Medusa](https://github.com/FasterDecoding/Medusa) - a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads
- [Self-RAG](https://selfrag.github.io/) - a new framework to train an arbitrary LM to learn to retrieve, generate, and critique to enhance the factuality and quality of generations, without hurting the versatility of LLMs
- [Mirascope](https://github.com/Mirascope/mirascope), [docs](https://docs.mirascope.io) - a toolkit for developing production-ready LLM-powered tools using Python and Pydantic
- [gateway](https://github.com/Portkey-AI/gateway) — route to 100+ open & closed source models with a unified API. It is also production-ready with support for caching, fallbacks, retries, timeouts, loadbalancing, and can be edge-deployed for minimum latency
- [corenet](https://github.com/apple/corenet) - a library for training deep neural networks for variety of tasks, including foundation models (e.g., CLIP and LLM), object classification, object detection, and semantic segmentation
- [MONSTER API](https://monsterapi.ai/) - a platform for no code LLM fine tuning and deployments
- [Lamini Platform](https://www.lamini.ai/) - a LLM platform that seamlessly integrates every step of the model refinement and deployment process – making model selection, model tuning and inference usage incredibly straightforward for your dev team
- [PowerInfer](https://github.com/SJTU-IPADS/PowerInfer) - a CPU/GPU LLM inference engine leveraging activation locality for your device
- [mixtral-offloading](https://github.com/dvmazur/mixtral-offloading) - efficient inference of Mixtral-8x7B models
- [bitnet.cpp](https://github.com/microsoft/BitNet) - is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support fast and lossless inference of 1.58-bit models on CPU (with NPU and GPU support coming next)
- [LayerSkip](https://github.com/facebookresearch/LayerSkip) - end-to-end solution promises to accelerate LLM generation times without the need for specialized hardware, by MetaAI
- [Lingua](https://github.com/facebookresearch/lingua) - a lean, efficient, and easy-to-hack codebase to research LLMs, by MetaAI
- [fairchem](https://github.com/facebookresearch/fairchem) - the FAIR Chemistry's centralized repository of all its data, models, demos, and application efforts for materials science and quantum chemistry, by MetaAI
- [LangExtract](https://github.com/google/langextract) - a python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization, by Google
- [Tinker](https://thinkingmachines.ai/blog/announcing-tinker/) - a flexible API for fine-tuning language models, by Thinking Machines
- [Petri](https://alignment.anthropic.com/2025/petri/) -  an open-source auditing tool to accelerate AI safety research, by Anthropics

### Agents
- [rowboat](https://github.com/rowboatlabs/rowboat) - AI-powered multi-agent builder, powered by OpenAI's Agents SDK
- [UI-TARS-1.5](https://seed-tars.com/1.5/) - an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds
- [A2A](https://github.com/google/A2A) - an open protocol enabling communication and interoperability between opaque agentic applications, by Google
- [Browser-Use](https://github.com/browser-use/browser-use) - enable AI to control your browser
- [smolagents](https://huggingface.co/blog/smolagents) - a very simple library that unlocks agentic capabilities for language models
- [OpenHands](https://github.com/All-Hands-AI/OpenHands) - a platform for software development agents powered by AI
- [Multi-Agent Orchestrator ](https://github.com/awslabs/multi-agent-orchestrator) - framework for managing multiple AI agents and handling complex conversations, by AWS
- [swarm](https://github.com/openai/swarm) - educational framework exploring ergonomic, lightweight multi-agent orchestration, by OpenAI
- [Agent-S](https://github.com/simular-ai/Agent-S) - an open agentic framework that uses computers like a human
- [TEN-Agent](https://github.com/TEN-framework/TEN-Agent) - a real-time multimodal agent integrated with the OpenAI Realtime API, RTC, and features weather checks, web search, vision, and RAG capabilities
- [bee-agent-framework](https://github.com/i-am-bee/bee-agent-framework) - open-source framework for building, deploying, and serving powerful agentic workflows at scale
- [agent.exe](https://github.com/corbt/agent.exe) - the easiest way to let Claude's new computer use capabilities take over your computer
- [Pearl](https://github.com/facebookresearch/pearl) - a production-ready RL AI Agent Library, by MetaAI
- [OpenAgents](https://github.com/xlang-ai/OpenAgents) - an open platform for using and hosting language agents in the wild of everyday life
- [agents](https://github.com/aiwaves-cn/agents) - an open-source library/framework for building autonomous language agents
- [ChatDev](https://github.com/OpenBMB/ChatDev) - highly customizable and extendable framework, which is based on LLMs and serves as an ideal scenario for studying collective intelligence
- [JARVIS-1](https://craftjarvis-jarvis1.github.io/) - Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models, generate sophisticated plans, and perform embodied control, within the open-world Minecraft universe
- [AppAgent](https://github.com/mnotgod96/AppAgent) - Multimodal Agents as Smartphone Users, an LLM-based multimodal agent framework designed to operate smartphone app
- [Kiln AI](https://kiln.tech) - Kiln is a free desktop app that provides tools for building AI products such as evals, RAG systems, agents, fine-tuning, synthetic data, and more, all without coding.

### AI Code Editors
- [Cursor](https://cursor.com/home) - 
- [Kiro](https://kiro.dev/) - 
- [Windsurf Editor](https://windsurf.com/editor) - 
- [Replit](https://replit.com/) - 
- [Kilo Code](https://kilocode.ai/) - AI coding agent for VS Code
- [Recurse](https://www.recurse.ml/) - custom semantic code checks for PRs
- [Jules](https://jules.google/) - asynchronous coding agent, by Google
- [Claude Code](https://www.anthropic.com/claude-code) - code onboarding, turn issues into PRs, make powerful edits
- [Claude Code Security Reviewer](https://github.com/anthropics/claude-code-security-review) - an AI-powered security review GitHub Action using Claude to analyze code changes for security vulnerabilities

## Devices
- [Reachy Mini](https://huggingface.co/blog/reachy-mini) - open-source robot designed for human-robot interaction, creative coding, and AI experimentation, by Hugging Face
- [Omi](https://www.omi.me/) - AI wearables that revolutionize how you capture and manage conversations
- [NotePin](https://www.plaud.ai/products/notepin) - wearable AI memory capsule, by Plaud
- [biped.ai](https://biped.ai/) - an AI wearable vest that helps blind and visually impaired people avoid obstacles, follow GPS instructions, and find crosswalks or door
- [LPU Inference Engine](https://groq.com/) - Language Processing Units, by Groq
- [FigureAI](https://www.figure.ai/) - AI robotics company bringing a general purpose humanoid to life
- [SanctuaryAI](https://sanctuary.ai/) - company on a mission to create the world’s first human-like intelligence in general-purpose robots
- [Mytra](https://mytra.ai/) - warehouse robotics
- [friend](https://www.friend.com/product.html) - AI-Powered Necklace companion designed not to help you get things done but to be there for you—anytime, anywhere
- [Limitless](https://www.limitless.ai/) - personalized AI powered by what you’ve seen, said, and heard
- [rabbit r1](https://www.rabbit.tech/) - a personalized operating system through a natural language interface
- [01 Project](https://www.openinterpreter.com/01) - the open-source language model computer, by Open Interpreter

### Glasses
- [Halo X](https://www.halo.so/) - smart glasses powered by Google Gemini and Perplexity AI that listen to, record, and transcribe every conversation around you
- [Aria Gen 2](https://ai.meta.com/blog/aria-gen-2-research-glasses-under-the-hood-reality-labs/) - a wearable device that combines the latest advancements in computer vision, machine learning, and sensor technology, by MetaAI
- [G1](https://www.evenrealities.com/g1) - , by Even Realities
- [AirGo Vision](https://solosglasses.com/) - Audio Smartglasses powered by ChatGPT, by Solosglasses
- [Ray-Ban Meta Smart Glasses](https://www.meta.com/smart-glasses/) - a 12 MP camera and five-mic system, [updates](https://about.fb.com/news/2024/04/new-ray-ban-meta-smart-glasses-styles-and-meta-ai-updates/), by Ray-Ban & MetaAI
- [Frame](https://brilliant.xyz/products/frame) - AI glasses designed to be worn as a pair of glasses with a suite of AI capabilities out of the box, by Brilliant Labs
- [air2](https://www.xreal.com/us/air2) - , by xreal
- [TCL RayNeo X2](https://www.rayneo.com/products/tcl-rayneo-x2) - AR Glasses, by RayNeo

## Income
- [Poe](https://techcrunch.com/2024/04/09/poe-introduces-a-price-per-message-revenue-model-for-ai-bot-creators/) - price-per-message revenue model for AI bot creators
- [GPTs Store](https://chat.openai.com/gpts) - create custom versions of ChatGPT that combine instructions, extra knowledge, and any combination of skills
- [Voice Library](https://elevenlabs.io/payouts) - share your voice in the Voice Library today and earn cash rewards when it's used
- [HuggingChat](https://huggingface.co/chat/) - making the community's best AI chat models available to everyone

## Tools
|Text-to-Image|Text-to-Music|Text-to-Video|Games|Brand|Prompt Generator|
:-:|:-:|:-:|:-:|:-:|:-:
[Midjourney](https://www.midjourney.com/)|[Mubert](https://mubert.com/)|[fal.ai](https://fal.ai/models)|[Leonardo.Ai](https://leonardo.ai/) - Assets|[Flair](https://flair.ai/)|[G-prompter](https://www.g-prompter.com/en)|
[Adobe Firefly](https://firefly.adobe.com/)|[Waveformer](https://waveformer.replicate.dev/)|[PIKA LABS](https://www.pika.art/)|[Dreamlab](https://dreamlab.gg/) - Animated Sprites|[Logolivery](https://logolivery.ai/)|[Prompt Builder](https://promptomania.com/midjourney-prompt-builder/)|
[Catbird](https://www.catbird.ai/)|[Morph Studio](https://www.morphstudio.com/)|[Kaiber](https://kaiber.ai/)|[Didimo](https://www.didimo.co/)||[Midjourney PromptHelper1](https://promptfolder.com/midjourney-prompt-helper/)|
[BlueWillow](https://www.bluewillow.ai/)||[Invideo](https://invideo.io/)|[Scenario](https://www.scenario.com/) - Assets||[Midjourney PromptHelper2](https://midjourney-prompt-helper.netlify.app/)|
[Lexica](https://lexica.art/)||[Moonvalley](https://moonvalley.ai/)|[Skybox](https://skybox.blockadelabs.com/) - World-building||[FlowGPT](https://flowgpt.com/)|
[Imgcreator](https://imgcreator.zmo.ai/)||[ilumine AI](https://ilumine.ai/)|[Bezi](https://www.bezi.com/ai) - 3D Assets|[Anthropic](https://console.anthropic.com/)|
[Craiyon](https://www.craiyon.com/)||[LTX Studio](https://ltx.studio/)|[Charmed](https://charmed.ai/) - 3D Assets|

## Text-to-image
||Models|
:-:|:-:
|Google|[Muse](https://muse-model.github.io/), [Imagen](https://imagen.research.google/), [Parti](https://sites.research.google/parti/), [HyperDreamBooth](https://hyperdreambooth.github.io/), [DreamBooth](https://dreambooth.github.io/)<br>[StyleDrop](https://styledrop.github.io/), [Imagen 2](https://deepmind.google/technologies/imagen-2/), [ImageFX](https://blog.google/technology/ai/google-labs-imagefx-textfx-generative-ai/), [Imagen 3](https://deepmind.google/technologies/imagen-3/)|
|OpenAI|[CLIP](https://openai.com/research/clip), [DALL·E](https://openai.com/research/dall-e), [DALL·E 2](https://openai.com/dall-e-2), [DALL·E 3](https://openai.com/dall-e-3), [4o Image Generation](https://openai.com/index/introducing-4o-image-generation)|
|[MetaAI](https://ai.meta.com/)|[CM3leon](https://ai.meta.com/blog/generative-ai-text-images-cm3leon), [Emu Video](https://emu-video.metademolab.com/), [Emu Edit](https://emu-edit.metademolab.com/), [Imagine](https://imagine.meta.com/)|
|[Stability.ai](https://stability.ai/news)|[Stable Diffusion XL](https://stability.ai/stable-diffusion), [DreamStudio](https://dreamstudio.ai/), [Clipdrop](https://clipdrop.co/), [DeepFloyd IF](https://stability.ai/blog/deepfloyd-if-text-to-image-model): ([Code](https://github.com/deep-floyd/IF), [Demo: HF](https://huggingface.co/spaces/DeepFloyd/IF))<br>[SDXL Turbo](https://stability.ai/news/stability-ai-sdxl-turbo), [Stable Cascade](https://stability.ai/news/introducing-stable-cascade), [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3), [Stable Diffusion 3 Medium](https://stability.ai/news/stable-diffusion-3-medium),<br>[Adversarial Diffusion Distillation](https://stability.ai/research/adversarial-diffusion-distillation), [Stable Diffusion 3.5](https://stability.ai/news/introducing-stable-diffusion-3-5), [Stable Diffusion 3.5 Large](https://stability.ai/news/sd3-5-large-controlnets)|
|Black Forest Labs|[FLUX.1](https://blackforestlabs.ai/#get-flux), [FLUX1.1 [pro]](https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/), [FLUX1.1 [pro] Ultra](https://blackforestlabs.ai/flux-1-1-ultra/), [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/)<br>[ FLUX Pro Finetuning API](https://blackforestlabs.ai/announcing-the-flux-pro-finetuning-api), [FLUX.1 Kontext](https://bfl.ai/announcements/flux-1-kontext), [FLUX.1 Krea [dev]](https://bfl.ai/announcements/flux-1-krea-dev)|
|[Playground](https://playground.com/)|[Playground v2](https://playground.com/blog/playground-v2), [Playground v3](https://playground.com/blog/introducing-playground-v3)|

- [UniDisc](https://unidisc.github.io/) - model, which is capable of jointly processing text and images for a variety of downstream tasks
- [Reve Image 1.0](https://preview.reve.art/) - a new model trained from the ground up to excel at prompt adherence, aesthetics, and typography
- [Frames](https://runwayml.com/research/introducing-frames) - an image generation model offering unprecedented stylistic control, by Runway
- [Ideogram](https://ideogram.ai/launch) - AI tools that will make creative expression more accessible, fun, and efficient
- [Kolors](https://huggingface.co/Kwai-Kolors/Kolors) - a large-scale text-to-image generation model based on latent diffusion, by the Kuaishou Kolors team
- [StoryDiffusion](https://storydiffusion.github.io/) - Consistent Self-Attention for Long-Range Image and Video Generation
- [Ilus AI](https://ilus.ai/) - AI illustration generator
- [Improving Diffusion Models for Authentic Virtual Try-on in the Wild](https://idm-vton.github.io/) - image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively
- [Distribution Matching Distillation](https://tianweiy.github.io/dmd/) - one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster
- [Generative Powers of Ten](https://powers-of-10.github.io/) - a method that uses a text-to-image model to generate consistent content across multiple image scales, enabling extreme semantic zooms into a scene, e.g., ranging from a wide-angle landscape view of a forest to a macro shot of an insect sitting on one of the tree branches
- [Delta Denoising Score](https://delta-denoising-score.github.io/) - a novel scoring function for text-based image editing that guides minimal modifications of an input image towards the content described in a target prompt
- [Prompt-to-Prompt](https://prompt-to-prompt.github.io/) - editing framework, where the edits are controlled by text only
- [OpenCLIP](https://github.com/mlfoundations/open_clip) - an open source implementation of OpenAI's CLIP (Contrastive Language-Image Pre-training)
- [LEDITS](https://editing-images-project.hf.space/) - combined lightweight approach for real-image editing, incorporating the Edit Friendly DDPM inversion technique with Semantic Guidance, thus extending Semantic Guidance to real image editing, while harnessing the editing capabilities of DDPM inversion
- [Würstchen](https://huggingface.co/blog/wuerstchen) - Fast Diffusion for Image Generation
- [ExactlyAI](https://exactly.ai) - create images in seconds with an AI that understands your style
- [ConceptLab](https://kfirgoldberg.github.io/ConceptLab/) - generative models have enabled us to transform our words into vibrant, captivating imagery
- [IP-Adapter](https://ip-adapter.github.io/) - Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models
- [MatchAI](https://match.color.io/) - a powerful web app that can copy the color grading from images so you can apply it to your own, by [color.io](https://www.color.io/)
- [Picogen](https://picogen.io/) - nonofficial API to Midjourney AI, Stability AI and DALLE-2 AI
- [FABRIC - Feedback via Attention-Based Reference Image Conditioning](https://sd-fabric.github.io/) - a technique to incorporate iterative feedback into the generative process of diffusion models based on StableDiffusion
- [Controlling Text-to-Image Diffusion by Orthogonal Finetuning (OFT)](https://oft.wyliu.com/) - for adapting text-to-image diffusion models to downstream tasks
- [InstructPix2Pix Learning to Follow Image Editing Instructions](https://www.timothybrooks.com/instruct-pix2pix/) - a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image
- [Composer](https://damo-vilab.github.io/composer-page/) - a large (5 billion parameters) controllable diffusion model trained on billions of (text, image) pairs. It can exponentially expand the control space through composition, leading to an enormous number of ways to generate and manipulate images, i.e., making the infinite use of finite means
- [GigaGAN: Large-scale GAN for Text-to-Image Synthesis](https://mingukkang.github.io/GigaGAN/) - changing texture with prompting, changing style with prompting, by Adobe Research
- [AI Image Generator](https://www.aiimagegenerator.org) - Free AI-powered text-to-image image generator

## Images
- [Qwen-Image-Edit](https://qwenlm.github.io/blog/qwen-image-edit) - the image editing version of Qwen-Image
- [EvoVLM-JP](https://sakana.ai/evolutionary-model-merge/) - drops image models to generate Japan’s traditional ukiyo-e artwork, by Sakana
- [PaintsUndo](https://lllyasviel.github.io/pages/paints_undo/) - A Base Model of Drawing Behaviors in Digital Paintings
- [SkyReels](https://skyreels.ai/) - generate comics from stories or files you upload
- [PhotoMaker](https://photo-maker.github.io/) - Customizing Realistic Human Photos via Stacked ID Embedding
- [NSF](https://light.princeton.edu/publication/nsf/) - Neural Spline Fields for Burst Image Fusion and Layer Separation
- [Material Palette](https://astra-vision.github.io/MaterialPalette/) - a method to extract Physically-Based-Rendering (PBR) materials from a single real-world image
- [DiffusionLight](https://diffusionlight.github.io/) - a simple yet effective technique to estimate lighting in a single input image
- [Magnific](https://magnific.ai/) - the image Upscaler & Enhancer
- [wasitai](https://wasitai.com/) - check if an image was generated by a machine
- [Textify](https://textify.storia.ai/) - a tool for replacing the gibberish in AI-generated images with your desired text
- [Interpolating between Images with Diffusion Models](https://clintonjwang.github.io/interpolation) - a method for zero-shot controllable interpolation using latent diffusion models
- [AnyDoor: Zero-shot Object-level Image Customization](https://damo-vilab.github.io/AnyDoor-Page/) - a diffusion-based image generator with the power to move target objects to new scenes at user-specified locations in a harmonious way
- [Matting Anything](https://chrisjuniorli.github.io/project/Matting-Anything/), [Code](https://github.com/SHI-Labs/Matting-Anything), [Demo: HF](https://huggingface.co/spaces/shi-labs/Matting-Anything) - an efficient and versatile framework for estimating the alpha matte of any instance in an image with user-prompt guidance
- [Plug-and-Play](https://pnp-diffusion.github.io/), [Code](https://github.com/MichalGeyer/plug-and-play) - a large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, allowing us to synthesize diverse images that convey highly complex visual concepts
- [Real-Time Neural Appearance Models](https://research.nvidia.com/labs/rtr/neural_appearance_models/) - a complete system for real-time rendering of scenes with complex appearance previously reserved for offline use, by NVIDIA
- [Designer](https://designer.microsoft.com/) - generate stunning designs and original images just by typing what you want. Get writing assistance and automatic layout suggestions for anything you add. [Designer expands preview with new AI design features](https://www.microsoft.com/en-us/microsoft-365/blog/2023/04/27/microsoft-designer-expands-preview-with-new-ai-design-features/), by Microsoft
- [Scribble Diffusion](https://scribblediffusion.com/) - turn your sketch into a refined image using AI
- [StudioGPT](https://www.latentlabs.art/) - a tool for reimagining an existing image
- [UnblurImage](https://unblurimage.ai/) - remove blur from photos and achieve stunning clarity online for free

### Watermarks
- [SynthID](https://deepmind.google/technologies/synthid/), [SynthID Text](https://huggingface.co/blog/synthid-text) - watermarks and identifies AI-generated content by embedding digital watermarks directly into AI-generated images, audio, text or video, by Google DeepMind and Hugging Face
- [Stable Signature](https://ai.meta.com/blog/stable-signature-watermarking-generative-ai/) - a new method for watermarking images, by MetaAI
- [ClixMagicAI](https://clixmagic.com/) - a professional AI-powered watermark removal tool
- [DeWatermark](https://dewatermark.ai/) - remove Watermark from photos online free with AI
- [UnwatermarkAI](https://unwatermark.ai/) - remove watermark from image and video with AI For Free, No Sign-up, No Ads

## Computer Vision
- [DINOv2](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/), [DINOv3](https://ai.meta.com/blog/dinov3-self-supervised-vision-model) - self-supervised learning for vision at unprecedented scale, by MetaAI
- [Depth-Anything](https://huggingface.co/spaces/LiheYoung/Depth-Anything) - a depth estimation solution that can deal with any images under any circumstance
- [TAO-Amodal](https://tao-amodal.github.io/) - benchmark is a dataset that includes amodal and modal bounding boxes for visible and occluded objects
- [OMG-Seg](https://github.com/lxtGH/OMG-Seg) - One Model that is Good enough to efficiently and effectively handle all the segmentation tasks, including image semantic, instance, and panoptic segmentation, as well as their video counterparts, open vocabulary settings, prompt-driven, interactive segmentation like SAM, and video object segmentation
- [PUG (Photorealistic Unreal Graphics)](https://pug.metademolab.com/) - 3 datasets for representation learning research
- [Tracking Anything in High Quality](https://github.com/jiawen-zhu/HQTrack) - a framework for high performance video object tracking and segmentation
- [DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data](https://dreamsim-nights.github.io/) - a new benchmark of synthetic image triplets that span a wide range of mid-level variations, labeled with human similarity judgments
- [CoTracker](https://co-tracker.github.io/), [CoTracker3](https://cotracker3.github.io/) - an architecture that jointly tracks multiple points throughout an entire video, by MetaAI
- [TAPIR](https://deepmind-tapir.github.io/) - a model for Tracking Any Point (TAP) that effectively tracks a query point in a video sequence, by Google DeepMind
- [DreamTeache](https://research.nvidia.com/labs/toronto-ai/DreamTeacher/) - a self-supervised feature representation learning framework that utilizes generative networks for pre-training downstream image backbones, by NVIDIA
- [ImageBind](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/), [Demo](https://imagebind.metademolab.com/demo), [Code](https://github.com/facebookresearch/ImageBind) - Image->Audio, Audio->Image, Text->Image&Audio, Aidio&Image->Image, Audio->Generated Image, by MetaAI
- [I-JEPA](https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa), [Code](https://github.com/facebookresearch/ijepa) - Image Joint Embedding Predictive Architecture is a method for self-supervised learning. At a high level, I-JEPA predicts the representations of part of an image from the representations of other parts of the same image, by MetaAI
- [Visual Prompting](https://landing.ai/what-is-visual-prompting) - an innovative approach that takes text prompting, used in applications such as ChatGPT, to computer vision
- [Tracking Everything Everywhere All at Once](https://omnimotion.github.io/) - a new test-time optimization method for estimating dense and long-range motion from a video sequence
- [Track-Anything](https://github.com/gaomingqi/Track-Anything) - a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only
- [EdgeSAM](https://mmlab-ntu.github.io/project/edgesam/) - an accelerated variant of the SAM, optimized for efficient execution on edge devices with minimal compromise in performance
- [EfficientSAM](https://yformer.github.io/efficient-sam/) - light-weight SAM models that exhibit decent performance with largely reduced complexity, by MetaAI
- [SAM](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/), [SAM2](https://ai.meta.com/blog/segment-anything-2/) - Segment Anything Model is a new AI model that can "cut out" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training, by MetaAI
- [Behind the Scenes: Density Fields for Single View Reconstruction](https://fwmb.github.io/bts/) - a neural network that predicts an implicit density field from a single image

## Video & Animation
- [SORA 2](https://openai.com/index/sora-2/), [SORA](https://openai.com/index/video-generation-models-as-world-simulators/) - video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects, by OpenAI
- [Matrix-Game 2.0](https://matrix-game-v2.github.io/) - an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion (high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS)
- [SkyReels V2](https://github.com/SkyworkAI/SkyReels-V2) - the model weights and inference code for our infinite-length film generative models
- [Wan2.1](https://github.com/Wan-Video/Wan2.1), [Wan2.2](https://github.com/Wan-Video/Wan2.2) -  Effective MoE Architecture; Cinematic-level Aesthetics; Complex Motion Generation; Efficient High-Definition Hybrid TI2V
- [Veo](https://deepmind.google/technologies/veo/veo-1/) - generates high-quality 1080p resolution videos in a wide range of cinematic and visual styles that can go beyond a minute; [Veo2](https://deepmind.google/technologies/veo/veo-2/) - creates videos with realistic motion and high quality output, up to 4K; [VideoFX](https://aitestkitchen.withgoogle.com/tools/video-fx) - a new experimental tool designed to help support creatives through the storytelling journey, by Google
- [GEN-1](https://runwayml.com/ai-magic-tools/gen-1/) & [Research](https://research.runwayml.com/gen1), [GEN-2](https://runwayml.com/ai-magic-tools/gen-2/) & [Research](https://research.runwayml.com/gen2), GEN-3-alpha & [Research](https://runwayml.com/blog/introducing-gen-3-alpha/), Gen-4 & [Research](https://runwayml.com/research/introducing-runway-gen-4) - a new frontier for high-fidelity, controllable video generation. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building [General World Models](https://research.runwayml.com/introducing-general-world-models), by Runway
- [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) - A Systematic Framework For Large Video Generation Model
- [LTX-Video](https://github.com/Lightricks/LTX-Video) - real-time AI video generation, open-source model, by Lightricks
- [AutoVFX](https://haoyuhsu.github.io/autovfx-website/) - Physically Realistic Video Editing from Natural Language Instructions
- [FacePoke](https://facepoke.net/) - real-time facial animation
- [X-Portrait 2](https://byteaigc.github.io/X-Portrait2/) - highly expressive portrait animation
- [Meta Movie Gen](https://ai.meta.com/research/movie-gen/) - our latest research breakthroughs demonstrate how you can use simple text inputs to produce custom videos and sounds, edit existing videos or transform your personal image into a unique video
- [Mochi 1](https://www.genmo.ai/play) - an open-source model for generating high-quality videos from text prompts, by genmo
- [Haiper](https://haiper.ai/) - simplifies video creation with text-to-video, image-to-video, and video enhancement options
- [Hailuo AI](https://hailuoai.video/) - Image-to-Video
- [Krea](https://www.krea.ai/) - generate images and videos (Luma, Runway, Kling, Hailuo, Pika) with a delightful AI-powered design tool
- [Pyramid Flow](https://pyramid-flow.github.io/) - a training-efficient Autoregressive Video Generation model based on Flow Matching
- [Videolulu](https://videolulu.com/) - create engaging content in popular formats for TikTok, Instagram, and YouTube
- [GoVidify](https://govidify.ai) - an AI-powered tool that turns your written content into short-form videos for TikTok, YouTube, and Instagram
- [hotshot](https://hotshot.co/release) - a large-scale diffusion transformer model that serves as the foundation for our upcoming consumer product
- [ClipAnything](https://www.opus.pro/clipanything) - the first-ever multimodal AI clipping that lets you clip any moment from any video using visual, audio, and sentiment cues, by Opus
- [Text2Infographic](https://text2infographic.com/) - converts your written content into eye-catching infographics without any need for design skills
- [Flow Studio](https://flowgpt.com/) - uses AI to transform your text prompts into visually captivating short films and videos
- [LivePortrait](https://liveportrait.github.io/) - Efficient Portrait Animation with Stitching and Retargeting Control
- [Odyssey](https://odyssey.systems/) - Hollywood-grade visual AI
- [VideoPoet](https://research.google/blog/videopoet-a-large-language-model-for-zero-shot-video-generation/) - a large language model for zero-shot video generation, by Google Reasearch
- [Character-1](https://www.hedra.com/) - model allows you to create lip-synced videos to any audio from a still image; imagine worlds, characters and stories with complete creative control, by Hedra
- [Showrunner](https://www.showrunner.xyz/) - AI platform designed to let you create an animated TV episode with just a prompt
- [Luma Dream Machine](https://lumalabs.ai/dream-machine) - an AI model that makes high quality, realistic videos fast from text and images, by Luma
- [Kling](https://kling.kuaishou.com/) - video generation with enhanced features and quality
- [ToonCrafter](https://doubiiu.github.io/projects/ToonCrafter/) - interpolate two cartoon images by leveraging the pre-trained image-to-video diffusion priors
- [VideoGigaGAN: Towards Detail-rich Video Super-Resolution](https://videogigagan.github.io/) - a generative VSR model that can produce videos with high-frequency details and temporal consistency, by Adobe Research
- [VASA-1](https://www.microsoft.com/en-us/research/project/vasa-1/) - Lifelike Audio-Driven Talking Faces Generated in Real Time, by Microdoft
- [MagicTime](https://pku-yuangroup.github.io/MagicTime/) - Time-lapse Video Generation Models as Metamorphic Simulators
- [Stable Video Diffusion](https://stability.ai/news/stable-video-diffusion-open-ai-video-model) - a foundation model for generative video based on the image model Stable Diffusion
- [EMO](https://humanaigc.github.io/emote-portrait-alive/) - Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions
- [LUMIERE](https://lumiere-video.github.io/) - A Space-Time Diffusion Model for Video Generation: Text-to-Video, Image-to-Video, Stylized Generation, Video Stylization, Cinemagraphs, Video Inpainting
- [ActAnywhere](https://actanywhere.github.io/) - Subject-Aware Video Background Generation
- [MagicVideo-V2](https://magicvideov2.github.io/) - integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline
- [I2VGen-XL](https://i2vgen-xl.github.io/) - High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models
- [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion) - an innovative diffusion pipeline designed for real-time interactive generation
- [WALT - Window Attention Latent Transformer](https://walt-video-diffusion.github.io/) - a transformer-based method for latent video diffusion models (LVDMs)
- [Hotshot](https://hotshot.co/) - GIF generator
- [Unscreen](https://www.unscreen.com/) - remove video background
- [Motrica](https://www.motorica.ai/) - technologies and tools for advanced character animation
- [CoDeF](https://qiuyu96.github.io/CoDeF/) - Content Deformation Fields for Temporally Consistent Video Processing
- [MagicEdit](https://magic-edit.github.io/) - supports various editing applications, including video stylization, local editing, video-MagicMix and video outpainting
- [To Infinity and Beyond](https://fablestudio.github.io/showrunner-agents/) - an approach to generating high-quality episodic content for IP's (Intellectual Property) using LLMs, custom state-of-the art diffusion models and our multi-agent simulation for contextualization, story progression and behavioral control
- [PlazmaPunk](https://www.plazmapunk.com/) - create your own music video with the power of AI
- Video-LLaMA, [Code](https://github.com/DAMO-NLP-SG/Video-LLaMA), [Demo: HF](https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA) - a multi-model LLM that achieves video-grounded conversations between humans and computers by connecting language decoder with off-the-shelf unimodal pre-trained models
- [AnimateDiff prompt travel](https://github.com/s9roll7/animatediff-cli-prompt-travel) - AnimateDiff with prompt travel + ControlNet + IP-Adapter
- [AnimateDiff](https://animatediff.github.io/), [Code](https://github.com/guoyww/AnimateDiff) - Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning
- [Animate-A-Story](https://videocrafter.github.io/Animate-A-Story/) - a video storytelling approach which can synthesize high-quality, structure-controlled, and character-controlled videos
- [Zeroscope](https://huggingface.co/spaces/fffiloni/zeroscope) - a watermark-free Modelscope-based video model optimized for producing high-quality 16:9 compositions and a smooth video output
- [Klap](https://klap.app/) - a tool that analyzes the video and finds short clips
- [Lalamu](https://lalamu.studio/demo) - low-quality video lip sync with preselected videos/video templates (take clips from videos, give the video new audio, and then the lips will sync up to that new audio within the video)
- [D-ID](https://www.d-id.com/) - uses generative AI to create customized videos featuring talking avatars at a touch of a button for businesses and creators.
- [Rooms.xyz](https://rooms.xyz) - create & remix interactive rooms from your browser
- [Wonder Dynamics](https://wonderdynamics.com/) - an AI tool that automatically animates, lights, and composes CG characters into a live-action scene
- [REVELxyz](https://www.revel.xyz/animai) - a tool for creating Animated Avatars from a single photo
- [ANIMATED DRAWINGS](https://sketch.metademolab.com/) - a tool that brings children's drawings to life, by animating characters to move around, by MetaAI
- [RERENDER A VIDEO](https://anonymous-31415926.github.io/), [Demo: HF](https://huggingface.co/spaces/Anonymous-sub/Rerender) - a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos
- Roop, [Code](https://github.com/s0md3v/roop) - take a video and replace the face in it with a face of your choice. You only need one image of the desired face
- [Text2Performer](https://yumingj.github.io/projects/Text2Performer.html) - Text-Driven Human Video Generation, where a video sequence is synthesized from texts describing the appearance and motions of a target performer
- [DragGAN](https://vcai.mpi-inf.mpg.de/projects/DragGAN/), [Code](https://github.com/XingangPan/DragGAN), [Demo: HF](https://huggingface.co/spaces/DragGan/DragGan) - way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc
- [DragDiffusion](https://yujun-shi.github.io/projects/dragdiffusion.html) - Harnessing Diffusion Models for Interactive Point-based Image Editing
- [In-N-Out: Face Video Inversion and Editing with Volumetric Decomposition](https://in-n-out-3d.github.io/) - our core idea is to represent the face in a video using two neural radiance fields, one for in-distribution and the other for out-of-distribution data, and compose them together for reconstruction
- [High-Resolution Video Synthesis with Latent Diffusion Models](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) - Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space, by NVIDIA

## 3D
- [Stable Point Aware 3D](https://stability.ai/news/stable-point-aware-3d) - real-time editing and complete structure generation of a 3D object from a single image, by by Stability AI
- [backflip](https://www.backflip.ai/) - AI 3D design tools for the physical world
- [cadwithai](https://www.cadwithai.com/) - a tool that allows users to create and edit CAD models using an AI chatbot to enhance efficiency and creativity in design work
- [Meshy](https://www.meshy.ai/) - create stunning 3D models with AI
- [Generative 3D API Toolkit](https://www.shutterstock.com/discover/generative-ai-3d) - generate 3D models, materials, and HDRIs at the speed of your imagination. Supercharge your 3D workflow with our groundbreaking Gen3D toolkit from Shutterstock powered by NVIDIA
- [Stable Fast 3D](https://stability.ai/news/introducing-stable-fast-3d) - generates high-quality 3D assets from a single image, by Stability AI
- [Stable Video 4D](https://stability.ai/news/stable-video-4d) - a single object video into multiple novel-view videos of eight different angles/views, by Stability AI
- [VGGHeads](https://github.com/KupynOrest/head_detector) - A Large-Scale Synthetic Dataset for 3D Human Heads
- [CharacterGen](https://huggingface.co/spaces/VAST-AI/CharacterGen)- Efficient 3D Character Generation from Single Images with Multi-View Pose Calibration
- [3D Gen](https://ai.meta.com/research/publications/meta-3d-gen) - fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes and textures in under a minut, by MetaAI
- [InstantMesh](https://github.com/TencentARC/InstantMesh) - Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models
- [Spline](https://spline.design/ai-generate) - Generate 3D objects from text prompts and images
- [SIMA](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/) - a Scalable Instructable Multiworld Agent (SIMA) that can follow natural-language instructions to carry out tasks in a variety of video game settings
- [Stable Video 3D](https://stability.ai/news/introducing-stable-video-3d) - Quality Novel View Synthesis and 3D Generation from Single Images, by Stability AI
- [TripoSR](https://stability.ai/news/triposr-3d-generation) - Fast 3D Object Generation from Single Images, by Stability AI
- [BlendNeRF](https://blandocs.github.io/blendnerf) - 3D-aware Blending with Generative NeRFs
- [4DGen](https://vita-group.github.io/4DGen/) - Grounded 4D Content Generation with Spatial-tempsoral Consistency
- [MobileBrick](https://code.active.vision/MobileBrick/) - Building LEGO for 3D Reconstruction on Mobile Devices. A novel data capturing and 3D annotation pipeline to obtain precise 3D ground-truth shapes without relying on expensive 3D scanners
- [PoseGPT](https://yfeng95.github.io/posegpt/) - Chatting about 3D Human Pose
- [ProlificDreamer](https://ml.cs.tsinghua.edu.cn/prolificdreamer/) - High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation
- [Stable Zero123](https://stability.ai/news/stable-zero123-3d-generation) - 3D Object Generation from Single Images, by Stability AI
- [SMERF](https://smerf-3d.github.io/) - Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration
- [DreamCraft3D](https://mrtornado24.github.io/DreamCraft3D/) - a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects
- [Genie](https://lumalabs.ai/genie) - 3D fundational model, by [Lumalabs](https://lumalabs.ai/)
- [Masterpiece X](https://www.masterpiecex.com/) - the generative text-to-3D app that allows users to create 3D objects and characters complete with mesh, texture, and animations
- [GAUSSIAN SPLAT](https://poly.cam/gaussian-splatting) - a rasterization technique for 3D reconstruction and rendering
- [SyncDreamer](https://github.com/liuyuan-pal/SyncDreamer) - generating multiview-consistent images from a single-view image
- [MAV3D (Make-A-Video3D)](https://make-a-video3d.github.io/) - a method for generating three-dimensional dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural Radiance Field (NeRF), which is optimized for scene appearance, density, and motion consistency by querying a Text-to-Video (T2V) diffusion-based model
- [HiFA](https://hifa-team.github.io/HiFA-site/) - High-fidelity Text-to-3D with Advanced Diffusion Guidance
- [AutoRecon](https://zju3dv.github.io/autorecon/) - a framework named for the automated discovery and reconstruction of an object from multi-view images
- [BITE](https://bite.is.tue.mpg.de/) - enables 3D shape and pose estimation of dogs from a single input image. The model handles a wide range of shapes and breeds, as well as challenging postures far from the available training poses, like sitting or lying on the ground
- [CSM (Common Sense Machines)](https://3d.csm.ai/) - generate your own textured 3D assets
- [MotionGPT: Human Motion as Foreign Language](https://motion-gpt.github.io/) - a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks
- [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360°](https://sizhean.github.io/panohead) - the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360° with diverse appearance and detailed geometry using only in-the-wild unstructured images for training
- [AvatarBooth](https://zeng-yifei.github.io/avatarbooth_page/) - a text-to-3D model. It creates an animatable 3D model with your word description. Also, it can generate customized model with 4~6 photos from your phone or a character design generated from diffusion model
- [Infinigen](https://infinigen.org/), [Code](https://github.com/princeton-vl/infinigen) - a procedural generator of 3D scenes, creating depth maps and labeling every aspect of the world it generates, by Princeton Vision & Learning Lab
- [USD - Universal Scene Description](https://developer.nvidia.com/usd) - an open and extensible framework and ecosystem for describing, composing, simulating and collaborating within 3D worlds, originally developed by Pixar Animation Studios
- [Shap-E: Demo](https://huggingface.co/spaces/hysts/Shap-E), [Code](https://github.com/openai/shap-e) - a conditional generative model for 3D assets, by OpenAI
- [Neural Kernel Surface Reconstruction](https://research.nvidia.com/labs/toronto-ai/NKSR/), [Code](https://github.com/nv-tlabs/nksr) - a novel method for reconstructing a 3D implicit surface from a large-scale, sparse, and noisy point, by NVIDIA
- [Neuralangelo](https://research.nvidia.com/labs/dir/neuralangelo/) - a framework for high-fidelity 3D surface reconstruction from RGB video captures. Using ubiquitous mobile devices, we enable users to create digital twins of both object-centric and large-scale real-world scenes with highly detailed 3D geometry, by NVIDIA
- [Rodin Diffusion](https://3d-avatar-diffusion.microsoft.com/) - a Generative Model for Sculpting 3D Digital Avatars, by Microsoft
- [3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) - three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 100 fps) novel-view synthesis at 1080p resolution
- [ConsistentNeRF](https://skhu101.github.io/ConsistentNeRF) - a method that leverages depth information to regularize both multi-view and single-view 3D consistency among pixels
- [Text2NeRF](https://eckertzhang.github.io/Text2NeRF.github.io/) - a text-driven 3D scene generation framework, combines the neural radiance field (NeRF) and a pre-trained text-to-image diffusion model to generate diverse view-consistent indoor and outdoor 3D scenes from natural language descriptions
- [Zip-NeRF](https://jonbarron.info/zipnerf/) - a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP
- [S-NeRF](https://ziyang-xie.github.io/s-nerf/) - a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly
- [Mip-NeRF 360](https://jonbarron.info/mipnerf360/) - Unbounded Anti-Aliased Neural Radiance Fields, an extension of mip-NeRF that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes
- [3D-aware Conditional Image Synthesis](https://www.cs.cmu.edu/~pix2pix3D/) - a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model synthesizes a photo from different viewpoints
- [Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior](https://make-it-3d.github.io/) - can create high-fidelity 3D content from only a single image
- [Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](https://lukashoel.github.io/text-to-room/) - generates textured 3D meshes from a given text prompt using 2D text-to-image models
- [Objaverse-XL](https://objaverse.allenai.org/) - an open dataset of over 10 million 3D objects
- [OmniObject3D](https://omniobject3d.github.io/) - a large vocabulary 3D object dataset with massive high-quality real-scanned 3D objects to facilitate the development of 3D perception, reconstruction, and generation in the real world

## Audio & Speech & Music
### [MetaAI](https://ai.meta.com/)
- [Spirit LM](https://speechbot.github.io/spiritlm/) - a foundation multimodal language model that freely mixes text and speech
- [Audiobox](https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts/) - generate voices and sound effects using a combination of voice inputs and natural language text prompts — making it easy to create custom audio for a wide range of use cases
- [Seamless](https://ai.meta.com/blog/seamless-communication/) - system that unlocks expressive cross-lingual communication in real time
- [SeamlessM4T](https://ai.meta.com/blog/seamless-m4t/) - a foundational multilingual and multitask model that seamlessly translates and transcribes across speech and text: automatic speech recognition, speech-to-text and speech-to-speech translation, text-to-text and text-to-speech translation
- [AudioCraft](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) - simple framework that generates high-quality, realistic audio and music from text-based user inputs after training on raw audio signals as opposed to MIDI or piano rolls
  - [MusicGen](https://ai.honu.io/papers/musicgen/), [Demo: HF](https://huggingface.co/spaces/facebook/MusicGen), [Code](https://github.com/facebookresearch/audiocraft) - a simple and controllable model for music generation
  - [AudioGen](https://felixkreuk.github.io/audiogen/) - an auto-regressive generative model that generates audio samples conditioned on text inputs
  - [EnCodec](https://ai.meta.com/blog/ai-powered-audio-compression-technique/) - a neural network that is trained end to end to reconstruct the input signal
- [MuAViC](https://ai.facebook.com/blog/muavic-audio-visual-speech-translation-benchmark/) - a Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation
- [Voicebox](https://voicebox.metademolab.com/) - Text-Guided Multilingual Universal Speech Generation at Scale

### Google
- [Music AI Sandbox](https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/) - an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music, powered by [Lyria2](https://deepmind.google/technologies/lyria/)
- [Lyria](https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/), [Lyria2](https://deepmind.google/technologies/lyria/) - AI music generation model that qdelivers high-fidelity music and professional-grade audio, capturing subtle nuances across a range of genres and intricate compositions
- [V2A](https://deepmind.google/discover/blog/generating-audio-for-video/) - video-to-audio research uses video pixels and text prompts to generate rich soundtracks
- [MusicFX](https://aitestkitchen.withgoogle.com/tools/music-fx) - a new experimental tool that enables users to generate their own music using AI
- [SingSong](https://storage.googleapis.com/sing-song/index.html) - a system which generates instrumental music to accompany input vocals
- [Translatotron 3](https://research.google/blog/unsupervised-speech-to-speech-translation-from-monolingual-data/) - unsupervised speech-to-speech translation from monolingual data
- [AudioPaLM](https://google-research.github.io/seanet/audiopalm/examples/) - a LLM for speech understanding and generation
- [MusicLM](https://google-research.github.io/seanet/musiclm/examples/), [Demo](https://aitestkitchen.withgoogle.com/experiments/music-lm) - a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff"
- [Universal Speech Model (USM)](https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html) - a state-of-the-art speech AI for 100+ languages

### [Eleven Labs](https://beta.elevenlabs.io/)
- [SFX v2](https://elevenlabs.io/sound-effects) - generate any sound imaginable from a text prompt
- [Eleven Music](https://elevenlabs.io/music) - generate studio-quality tracks instantly, any genre, any style, vocals or instrumental, in minutes using simple text prompts
- [11ai](https://11.ai/) - a personal AI voice assistant, built with ElevenLabs Conversational AI
- [Flash](https://elevenlabs.io/blog/meet-flash) - a newest model that generates speech in 75ms + application & network latency
- [xtovoice](https://www.xtovoice.com/) - analyze your X profile to generate a unique voice using ElevenLabs
- [Sound Effects](https://elevenlabs.io/sound-effects) - create distinctive sound effects directly from text descriptions, streamlining your audio production process
- [Dubbing Studio](https://elevenlabs.io/blog/introducing-dubbing-studio/) - a tool, enabling automatic, end-to-end video translation across 29 languages. hands-on control over transcript, translation, timing, and more
- [Speech to Speech](https://elevenlabs.io/blog/product-update-speech-to-speech-and-changes-to-voices/) - a tool that lets you turn the recording of one voice to sound as if spoken by another
- [Eleven Multilingual v2](https://elevenlabs.io/blog/multilingualv2/) - a Foundational AI Speech Model for Nearly 30 Languages
- [Eleven Multilingual v1](https://beta.elevenlabs.io/blog/eleven-multilingual-v1/), [Demo](https://beta.elevenlabs.io/?ref=beta.elevenlabs.io) - generate top-quality spoken audio in any voice and style with the most advanced and multipurpose AI speech tool out there
- [AI Speech Classifier](https://beta.elevenlabs.io/blog/ai-speech-classifier/), [Demo](https://beta.elevenlabs.io/ai-speech-classifier) - detect whether an audio clip was created using ElevenLab

### Other
- [Stable Audio](https://stability.ai/blog/stable-audio-using-ai-to-generate-music), [Stable Audio 2.0](https://stability.ai/news/stable-audio-2-0), [Stable Audio 2.5](https://stability.ai/news/stability-ai-introduces-stable-audio-25-the-first-audio-model-built-for-enterprise-sound-production-at-scale) - audio generation model designed specifically for enterprise-grade sound production, by Stability AI
- [Stable Audio Open](https://stability.ai/news/introducing-stable-audio-open) - an open source text-to-audio model for generating up to 47 seconds of samples and sound effects, by Stability AI
- [Voxtral](https://mistral.ai/news/voxtral) - state‑of‑the‑art speech understanding models are available in two sizes—a 24B variant for production-scale applications and a 3B variant for local and edge deployments, by Mistral
- [chatterbox](https://github.com/resemble-ai/chatterbox) - SoTA open-source TTS, by Resemble AI
- [dia](https://github.com/nari-labs/dia) - a 1.6B parameter text to speech model capable of generating ultra-realistic dialogue in one pass, by Nari Labs
- [Amazon Nova Sonic](https://aws.amazon.com/ai/generative-ai/nova/speech/) - a state-of-the-art speech-to-speech model that delivers real-time, human-like voice conversations with industry-leading price performance and low latency
- [MoshiVis](https://kyutai.org/moshivis) - an open-source Vision Speech Model (VSM) with the same low-latency and natural conversation skills as Moshi, with the additional ability to discuss visual inputs
- [Conversational Speech Model (CSM)](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice) - an end-to-end multimodal learning system designed to generate more natural and contextually appropriate AI speech, by Sesame
- [Di♪♪Rhythm](https://github.com/ASLP-lab/DiffRhythm) - Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion
- [YuE](https://map-yue.github.io/) - Open Full-song Music Generation Foundation Model, something similar to Suno.ai but open
- [Riffusion](https://www.riffusion.com/) is an AI-powered music generator that turns text into sound
- [GitPodcast](https://www.gitpodcast.com/) - turn any GitHub repository into an engaging podcast in seconds
- [Qwen2-Audio](https://qwenlm.github.io/blog/qwen2-audio) - capable of accepting audio and text inputs and generating text outputs
- [Neutone Morpho](https://neutone.ai/morpho) - pre-trained AI models you can transform any incoming audio into the characteristics, or “style”, of the sounds that the model is based on
- [Lazybird](https://www.lazybird.app/) - AI-powered voice over generator – perfect for videos, podcasts, audiobooks, and educational content
- [AI Jukebox](https://huggingface.co/spaces/enzostvs/ai-jukebox) - a free in-browser text-to-music generation tool
- [Chatter](https://chatter.hume.ai/) - an interactive podcast, by Hume
- [OpenVoice](https://huggingface.co/spaces/myshell-ai/OpenVoice), [OpenVoice2](https://huggingface.co/myshell-ai/OpenVoiceV2) - a versatile instant voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages
- [Voice Engine](https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices) - a model for creating custom voices, by OpenAI 
- [Udio](https://www.udio.com/) - discover, create, and share music with the world
- [Image to SFX](https://huggingface.co/spaces/fffiloni/Image2SFX-comparison) - compare sound effects generation models from image caption
- [DubbingAI](https://dubbing-ai.com/) - AI tool can convert your voice into high-quality cloned voices—from celebrities to your favorite gaming characters—in real time
- [StockMusic](https://www.stockmusic.app/) - a platform for AI-generated tunes that allows you to generate up to 10 minutes of copyright-free music
- [RIFFUSION](https://www.riffusion.com/about) - the model to generate images of spectrograms and can then be converted to an audio clip
- [CLAP](https://github.com/LAION-AI/CLAP/) - you can extract a latent representation of any given audio and text for your own model, or for different downstream task
- [Vscoped](https://vscoped.com/) - effortlessly transcribe your video content to boost click-through rates and watch time
- MERT, [Code](https://github.com/yizhilll/MERT), [Demo: HF](https://huggingface.co/spaces/m-a-p/Music-Descriptor) - an Acoustic Music Understanding Model with Large-Scale Self-supervised Training
- [Ecoute](https://github.com/SevaSk/ecoute) - a live transcription tool that provides real-time transcripts for both the user's microphone input (You) and the user's speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI's GPT-3.5 for the user to say based on the live transcription of the conversation
- [SadTalker: Demo](https://huggingface.co/spaces/vinthony/SadTalker) - Stylized Audio-Driven Single Image Talking Face Animation
- [Recast](https://www.letsrecast.ai/) - turn your want-to-read articles into rich audio summaries
- AudioGPT, [Demo: HuggingFace](https://huggingface.co/spaces/AIGC-Audio/AudioGPT), [Code](https://github.com/AIGC-Audio/AudioGPT) - Understanding and Generating Speech, Music, Sound, and Talking Head
- [Chirp](https://suno-ai.notion.site/Chirp-Examples-f05351485da74d769d6183220a6e5da7) - music model, generates realistic audio - including speech, music and sound effects
- [Bark](https://github.com/suno-ai/bark) - a transformer-based text-to-audio model created, by [Suno](https://www.suno.ai/). Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communication like laughing, sighing and crying
- [Whisper](https://openai.com/research/whisper) - an automatic speech recognition (ASR) system, that approaches human level robustness and accuracy on English speech recognition
- [Musicfy](https://www.musicfy.lol/) - music like you've never heard. Create and discover AI covers of your favorite songs
- [Jukebox](https://openai.com/research/jukebox) - a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles, by OpenAI
- [Koe Recast](https://koe.ai/) - transform your voice using AI

## Code & Math
||Code|Math|
:-:|:-:|:-:
|[Mistral AI](https://mistral.ai/)|[Codestral](https://mistral.ai/news/codestral/), [Codestral Mamba](https://mistral.ai/news/codestral-mamba/), [Codestral 25.01](https://mistral.ai/news/codestral-2501), [Devstral](https://mistral.ai/news/devstral), [Mistral Code](https://mistral.ai/news/mistral-code)|[MathΣtral](https://mistral.ai/news/mathstral/)|
|Stablility AI|[StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding), [Stable Code 3B](https://stability.ai/news/stable-code-2024-llm-code-completion-release), [Stable Code Instruct 3B](https://stability.ai/news/introducing-stable-code-instruct-3b)||
|Google DeepMind||[FunSearch](https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/), [alphageometry](https://github.com/google-deepmind/alphageometry)|
|Salesforce|[CodeT5 & CodeT5+](https://github.com/salesforce/CodeT5/tree/main), [CodeGen2.5](https://blog.salesforceairesearch.com/codegen25/)||
|Alibaba Cloud|[CodeQwen1.5](https://qwenlm.github.io/blog/codeqwen1.5/), [Qwen2.5-Coder](https://qwenlm.github.io/blog/qwen2.5-coder), [Qwen3-Coder](https://qwenlm.github.io/blog/qwen3-coder/)|[Qwen2-Math](https://qwenlm.github.io/blog/qwen2-math/), [Qwen2.5-Math](https://qwenlm.github.io/blog/qwen2.5-math/)|
|DeepSeek||[DeepSeek-Prover-V2](https://github.com/deepseek-ai/DeepSeek-Prover-V2)|

- [CodeMender](https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security) - an AI-powered agent that improves code security automatically, by Google
- [Opal](https://developers.googleblog.com/en/introducing-opal) - an experimental tool that lets you build and share powerful AI mini apps that chain together prompts, models, and tools, by Google
- [Codex](https://openai.com/index/introducing-codex) - a cloud-based software engineering agent that can work on many tasks in parallel, powered by codex-1; [codex-cli](https://github.com/openai/codex) - lightweight coding agent that runs in your terminal, by OpenAI
- [DeepCoder](https://www.together.ai/blog/deepcoder), [GitHub](https://github.com/agentica-project/rllm) - a code reasoning model finetuned from Deepseek-R1-Distilled-Qwen-14B via distributed RL, by Agentica team and Together AI
- [AlphaEvolve](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms) - a Gemini-powered an agentic system that used LLMs to generate code in an evolutionary process, by Google DeepMind
- [SWE-1](https://windsurf.com/blog/windsurf-wave-9-swe-1) -  a family of models optimized for the entire software engineering process, not just the task of coding, by Windsurf
- [DeepWiki](https://deepwiki.com/) - understand unfamiliar codebases by automatically generating architecture diagrams, documentation, and source code links for public GitHub repositories, by Devin
- [Devin](https://www.cognition-labs.com/introducing-devin), [Devin 2.0](https://cognition.ai/blog/devin-2) - a new agent-native IDE experience for working with Devin
- [Sleek](https://sleek.design/) - tool that generate a sleek landing page with AI in minutes
- [MathGPT](https://math-gpt.org/) - simplifies math problems with step-by-step solutions
- [neo](https://heyneo.so/blog) - a fully autonomous Machine Learning Engineer
- [bolt.new](https://bolt.new/) - prompt, run, edit, and deploy full-stack web apps
- [Genie](https://cosine.sh/genie) - AI software engineer - achieving a 30% eval score on the industry standard benchmark SWE-Bench. Genie is a fine-tuned version of GPT-4o with a larger context window of undisclosed size. Genie is able to solve bugs, build features, refactor code, and everything in between either fully autonomously or paired with the user, like working with a colleague, not just a copilot
- [The AI Scientist](https://github.com/SakanaAI/AI-Scientist) - Towards Fully Automated Open-Ended Scientific Discovery
- [Dracarys](https://huggingface.co/abacusai/Dracarys-72B-Instruct) - a new family of open LLMs for coding, by Abacus.AI
- [MathPile](https://gair-nlp.github.io/MathPile/) - a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens
- [magicoder](https://github.com/ise-uiuc/magicoder) - a model family empowered by OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code
- [LearnLM](https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/) - a family of models fine-tuned for learning, and grounded in educational research to make teaching and learning experiences more active, personal and engaging, by Google
- [Llemma](https://github.com/EleutherAI/math-lm) - an open language model for mathematics (repository also contains submodules related to the overlap, fine-tuning, and theorem proving experiments described in the paper)
- [AlphaCodium](https://github.com/Codium-ai/AlphaCodium) - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems
- [sketch-2-app](https://www.sketch2app.io/examples) - generate code based on sketch
- [GPT Pilot](https://github.com/Pythagora-io/gpt-pilot) - a true AI developer that writes code, debugs it, talks to you when it needs help, etc
- [MAmmoTH](https://tiger-ai-lab.github.io/MAmmoTH/) - a series of open-source LLMs specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset
- [WrenAI](https://github.com/Canner/WrenAI) - open-source Text-to-SQL solutionf or data teams to get results and insights faster by asking business questions without writing SQL
- [Defog](https://defog.ai/blog/open-sourcing-sqlcoder/) - a state-of-the-art LLM for converting natural language questions to SQL queries, which outperforms major open-source models and slightly outperforms gpt-3
- [v0](https://v0.dev/) - a generative user interface system. It generates copy-and-paste friendly React code based on Shadcn UI and Tailwind CSS that people can use in their projects, by Vercel Labs
- [SafeCoder](https://huggingface.co/blog/safecoder) - a code assistant solution built for the enterprise. In marketing speak: “your own on-prem GitHub copilot”, by Hugging Face
- [Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/) - a state-of-the-art LLM capable of generating code, and natural language about code, from both code and natural language prompts, by MetaAI
- [Teaching Arithmetic to Small Transformers](https://github.com/lee-ny/teaching_arithmetic) - small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective
- [InterCode](https://intercode-benchmark.github.io/) - framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations
- [LeanDojo](https://leandojo.org/) - set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research
- [GPT Engineer](https://github.com/AntonOsika/gpt-engineer) - is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt
- [CodeTF](https://github.com/salesforce/CodeTF) - a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on. It aims to facilitate easy integration of SOTA CodeLLMs into real-world applications
- [Let’s Verify Step by Step](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision#fn-1) - a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”), by OpenAI
- [🦍 Gorilla: LLM Connected with Massive APIs](https://gorilla.cs.berkeley.edu/) - a finetuned LLaMA-based model that surpasses GPT-4 on writing API calls
- [Framer](https://www.framer.com/) - a tool that constructs a completely unique website for you based on a text prompt
- [Pico](https://picoapps.xyz/) - a tool that use GPT4 to instantly build simple, shareable web apps
- [dropbase](https://github.com/DropbaseHQ/) - uild and prototype web apps faster with AI

## Games
- [Genie](https://sites.google.com/view/genie-2024/), [Genie 2](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/), [Genie 3](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/) - a general purpose world model that can generate an unprecedented diversity of interactive environment, by Google DeepMind
- [Muse](https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation) - the first World and Human Action Model (WHAM), a generative AI model of a video game that can generate game visuals, controller actions, or both, by Microsoft
- [GenChess](https://labs.google/genchess) - turns your ideas into playable art pieces using Google’s Imagen 3 model
- [ExistAI](https://exists.ai/) - games from text
- [PokemonRedExperiments](https://github.com/PWhiddy/PokemonRedExperiments) - train RL agents to play Pokemon Red
- [BitMagic](https://bitmagic.games/) - game creation
- [AI Town](https://github.com/a16z-infra/AI-town) - a deployable starter kit for building and customizing your own version of AI town - a virtual town where AI characters live, chat and socialize
- [Generative Agents: Interactive Simulacra of Human Behavior](https://github.com/joonspk-research/generative_agents) - contains our core simulation module for generative agents—computational agents that simulate believable human behaviors—and their game environment
- [STEVE-1](https://sites.google.com/view/steve-1) - a Generative Model for Text-to-Behavior in Minecraft
- [Mastering Stratego](https://www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information) - DeepNash, an AI agent that learned the game from scratch to a human expert level by playing against itself
- [Voyager: An Open-Ended Embodied Agent with LLMs](https://voyager.minedojo.org/) - the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention

## Robotics
- [Gemini Robotics 1.5](https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/), [Gemini Robotics](https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/) - vision-language-action (VLA) model turns visual information and instructions into motor commands for a robot to perform a task, by by Google DeepMind
- [V-JEPA 2](https://about.fb.com/news/2025/06/our-new-model-helps-ai-think-before-it-acts/), [V-JEPA](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) - a new world model that achieves state-of-the art visual understanding and prediction in the physical world, improving the physical reasoning of AI agents, by MetaAI
- [Helix](https://www.figure.ai/news/helix) - a generalist Vision-Language-Action (VLA) model that unifies perception, language understanding, and learned control to overcome multiple longstanding challenges in robotics
- [ASAP](https://agile.human2humanoid.com/) - Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills
- [π0](https://www.physicalintelligence.company/blog/pi0), [Open Sourcing π0](https://www.physicalintelligence.company/blog/openpi) - a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables, by Physical Intelligence (π)
- [Genesis](https://genesis-embodied-ai.github.io/) - a comprehensive physics simulation platform designed for general purpose Robotics, Embodied AI, & Physical AI applications
- [Open-TeleVision](https://robot-tv.github.io/) - an open-sourced immersive teleoperation system with stereo visual feedback. Robots executing highly precise, extremely long-horizon tasks with high success rate, autonomously
- [unitree_il_lerobot](https://github.com/unitreerobotics/unitree_IL_lerobot) - open-source project is a modification of the LeRobot open-source training framework, enabling the training and testing of data collected using the dual-arm dexterous hands of Unitree's G1 robot
- [LeRobot](https://github.com/huggingface/lerobot) - aims to provide models, datasets, and tools for real-world robotics in PyTorch
- [DrEurek](https://eureka-research.github.io/dr-eureka/) - Language Model Guided Sim-To-Real Transfer
- [UniSim](https://universal-simulator.github.io/unisim/) - a real-world simulator range from controllable content creation in games and movies to training embodied agents purely in simulation that can be directly deployed in the real world
- [JAT (Jack of All Trades)](https://huggingface.co/blog/jat) - a transformer-based agent capable of playing video games, controlling a robot to perform a wide variety of tasks, understanding and executing commands in a simple navigation environment
- [Dobb·E](https://dobb-e.com/) - an open-source, general framework for learning household robotic manipulation
- [OpenEQA](https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/) - from word models to world models, by MetaAI
- [Mobile ALOHA](https://mobile-aloha.github.io/) - Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation, by Stanford
- [AutoRT, SARA-RT and RT-Trajectory](https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/) - by Google DeepMind
- [Robot Parkour Learning](https://robot-parkour.github.io/) - a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data
- [Open X-Embodiment](https://robotics-transformer-x.github.io/) - Robotic Learning Datasets and RT-X Models
- [Eureka](https://eureka-research.github.io/) - a human-level reward design algorithm powered by LLMs, by NVIDIA
- [Language to rewards for robotic skill synthesis](https://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html) - an approach to teaching robots novel actions through natural language input is proposed, using reward functions as an interface to bridge the gap between language and low-level robot actions
- [VIMA](https://vimalabs.github.io/) - General Robot Manipulation with Multimodal Prompts
- [RT-2](https://robotics-transformer2.github.io/) - a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for robotic control, by Google DeepMind
- [Robots That Ask For Help](https://robot-help.github.io/) - a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed
- [ViNT: A Foundation Model for Visual Navigation](https://visualnav-transformer.github.io/) - a goal-conditioned navigation policy trained on diverse, cross-embodiment training data, and can control many different robots in zero-shot
- [Navigating to Objects in the Real World](https://theophilegervet.github.io/projects/real-world-object-navigation/) - 
- [RVT: Robotic View Transformer](https://robotic-view-transformer.github.io/) - a multi-view transformer for 3D manipulation that is both scalable and accurate. RVT takes camera images and task language description as inputs and predicts the gripper pose action, by NVIDIA
- [TidyBot](https://tidybot.cs.princeton.edu/) - personalized Robot Assistance with Large Language Models
- [Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning](https://sites.google.com/view/op3-soccer) - by OP3 Soccer Team, by Google DeepMind
- [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/) - embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts
- [Scaling Robot Learning with Semantically Imagined Experience](https://diffusion-rosie.github.io/) - 
- [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/) - low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface

## Typography
- [GenType](https://labs.google/gentype) - make an alphabet out of anything, by Google
- [Fontjoy](https://fontjoy.com/) - uses deep learning algorithms to suggest font pairings that balance style and readability
- [ControlNet](https://huggingface.co/DionTimmer/controlnet_qrcode), [Demo: HF](https://huggingface.co/spaces/huggingface-projects/QR-code-AI-art-generator), [How to make a QR code with Stable Diffusion](https://stable-diffusion-art.com/qr-code/) - QR Code Conditioned ControlNet Models for Stable Diffusion. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape
- [Word-As-Image for Semantic Typography](https://wordasimage.github.io/Word-As-Image-Page/) - A few examples of our Word-As-Image illustrations in various fonts and for different textual concept. The semantically adjusted letters are created completely automatically using our method, and can then be used for further creative design as we illustrate here
- [DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion](https://ds-fusion.github.io/) - create artistic typography automatically, a novel method to automatically generate artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word, while ensuring that the output remains readable

## Bio & Med
- [AlphaGenome](https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/) - an AI tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes, by Google
- [TxGemma](https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/) - a LLM designed to improve the efficiency of therapeutic development, from identifying promising targets to helping predict clinical trial outcomes, by Google
- [DolphinGemma](https://blog.google/technology/ai/dolphingemma/) - a LLM is helping scientists study how dolphins communicate — and hopefully find out what they're saying, by Google
- [AI co-scientist](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist) - a multi-agent AI system built with Gemini 2.0 as a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals, and to accelerate the clock speed of scientific and biomedical discoveries, by Google
- [BioEmu-1](https://www.microsoft.com/en-us/research/blog/exploring-the-structural-changes-driving-protein-function-with-bioemu-1) - exploring the structural changes driving protein function
- [AlphaFold 3](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/), [Code](https://github.com/google-deepmind/alphafold3) - an AI model that predict the structure of proteins, DNA, RNA, ligands and more, and how they interact, by Google DeepMind and Isomorphic Labs
- [AMIE](https://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html) - a research AI system for diagnostic medical reasoning and conversations, by Google
- [MentalLLaMA](https://github.com/SteveKGYang/MentalLLaMA) - mental health analysis with LLMs
- [AlphaMissense](https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/) - an AI model classifying missense variants to help pinpoint the cause of diseases, by Google DeepMind
- [meditron](https://github.com/epfLLM/meditron) - a suite of open-source medical LLM adapted to the medical domain from Llama-2 through continued pretraining on a comprehensively curated medical corpus, including selected PubMed papers and abstracts, a new dataset of internationally-recognized medical guidelines, and a general domain corpus
- [evodiff](https://github.com/microsoft/evodiff) - combines evolutionary-scale data with diffusion models for controllable protein sequence generation
- [SAM-Med2D](https://github.com/OpenGVLab/SAM-Med2D) - applying the Segment Anything Model (SAM) to medical 2D images
- [Med-Flamingo](https://huggingface.co/med-flamingo/med-flamingo) - a medical vision-language model with multimodal in-context learning abilities
- [Brain2Music](https://google-research.github.io/seanet/brain2music/) - Reconstructing Music from Human Brain Activity
- [Seeing the World through Your Eyes](https://world-from-eyes.github.io/) - reconstruct a 3D scene beyond the camera's line-of-sight using portrait images containing eye reflections
- [Mind-Video](https://mind-video.com/) - High-quality Video Reconstruction from Brain Activity
- [Med-PaLM](https://sites.research.google/med-palm/) - a large language model (LLM) designed to provide high-quality answers to medical questions
- [PMC-LLaMA](https://github.com/chaoyi-wu/PMC-LLaMA) - the official codes for "PMC-LLaMA: Continue Training LLaMA on Medical Papers"

## Science
- [AlphaEarth Foundations](https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/) - an artificial intelligence (AI) model that functions like a virtual satellite, by Google DeepMind
- [MatterGen](https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/) - a generative AI tool that tackles materials discovery from a different angle. Instead of screening the candidates, it directly generates novel materials given prompts of the design requirements for an application, by Microsoft
- [GNoME](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/) - DL tool that dramatically increases the speed and efficiency of discovery by predicting the stability of new materials, by Google DeepMind
- [AlphaQubit](https://blog.google/technology/google-deepmind/alphaqubit-quantum-error-correction/) - AI system accurately identifies errors inside quantum computers, helping to make this new technology more reliable, by Google DeepMind

### Climat
- [Weather Lab](https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai), [app](https://deepmind.google.com/science/weatherlab) - experimental cyclone predictions, by Google DeepMind
- [Planet Parasol](https://www.planetparasol.ai/) - simulating the impacts of Stratospheric Aerosol Injection (SAI) deployment
- [GraphCast](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/) - AI model for faster and more accurate global weather forecasting, by Google DeepMind
- [OpenDAC](https://open-dac.github.io/) - a research project aimed at significantly reducing the cost of Direct Air Capture (DAC), by FAIR at Meta and Georgia Tech
- [MetNet-3](https://research.google/blog/metnet-3-a-state-of-the-art-neural-weather-model-available-in-google-products/) - the first AI weather model to learn from sparse observations and outperform the top operational systems up to 24 hours ahead at high resolutions. A portion of its forecasts are now available across various Google products, by Google
- [ClimaX A foundation model for weather and climate](https://microsoft.github.io/ClimaX/) - a flexible and generalizable deep learning model for weather and climate science. [Introducing ClimaX: The first foundation model for weather and climate](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/introducing-climax-the-first-foundation-model-for-weather-and-climate/)

## Military
- [AIP Pillars](https://www.palantir.com/platforms/aip/) - activate LLMs and other AI on your private network, subject to full control, by Palantir
- [GeoSpy](https://geospy.ai/) - upload satellite or aerial images, and GeoSpy’s AI examines visual details like landmarks, terrain features, and vegetation patterns to provide precise location predictions

## Other: Fin, Presentation
- [AI Sheets](https://huggingface.co/blog/aisheets) - open-source tool for building, enriching, and transforming datasets using AI models with no code, by Hugging Face
- [AI Spreadsheet](https://sourcetable.com) - analyze data and complete tasks in seconds with Sourcetable's Excel & data analyst
- [TacticAI](https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics/) - an AI assistant for football tactics, by Google DeepMind
- [FactSnap](https://factsnap.nand.io/) - reliable fact-checking companion. Verify information while browsing the web with the Chrome extension
- [Bricks](https://www.thebricks.com/) - an AI-powered tool that generates reports, visuals, and presentations from your data
- [Learn About](https://learning.google.com/experiments/learn-about/signup) - generates tailored educational experiences based on user questions and uploaded materials
- [Hautech AI](https://www.hautech.ai/) - an AI platform that turns simple clothing images into professional-grade fashion photos
- [Atlas](https://www.atlas.org/) - a school AI assistant that provides personalized help by studying your specific class materials
- [Ollie](https://ollie.ai/) - delivers new and favorite recipes written just for you each week
- [Food Mood](https://artsandculture.google.com/experiment/food-mood/HwHnGalZ3up0EA) - a fusion recipe generator powered, by Google
- [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) - an open-source financial LLMs
- [guidde](https://www.guidde.com/) - create documentation/presentation/FAQ from captured video
- [Gamma](https://gamma.app/) - create visually appealing presentations
- [Tome](https://tome.app/) - create a compelling starting point for your presentation in minutes