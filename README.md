# LLMs Tools & Research Projects
The repository contains a list of ready-to-use AI Tools, Open Sources, and Research Projects \
Apart from LLMs, you can find here new AI research from other areas such as Computer Vision, etc.\
Welcome to contribute.

## Large Language Models (LLMs) and Chatbots
[DeepLearning.AI Short Courses | Andrew Ng](https://learn.deeplearning.ai/) - short courses about LLMs\
[The Inside Story of ChatGPT‚Äôs Astonishing Potential | Greg Brockman | TED](https://youtu.be/C_78DM8fG6E)\
[State of GPT | Andrej Karpathy | Video](https://youtu.be/bZQun8Y4L2A)\
[Opportunities in AI - 2023 | Andrew Ng | Video](https://youtu.be/5p248yoa3oE?si=LJQTHeOF-XUQB72U)\
[GPT-4 Turbo| OpenAI DevDay, Opening Keynote| Sam Altman | Video](https://www.youtube.com/live/U9mJuUkhUzk?si=f4qPQh0_buASTp1b)\
[The Rise and Rise of A.I. LLMs & their associated bots like ChatGPT| Visualization](https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt)

<!---
||Google|OpenAI|Meta|EleutherAI|Stability AI|Anthropic|
:-:|:-:|:-:|:-:|:-:|:-:|:-:
2023|[PaLM-2](https://ai.google/discover/palm2)<br>[Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/)|[GPT-4](https://openai.com/product/gpt-4)|LLAMA2<br>[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)|[Pythia](https://github.com/EleutherAI/pythia)|[Stable Vicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)<br>[StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)|[Claude2](claude.ai)<br>[Claude](https://www.anthropic.com/product)|
2022|[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)<br>[GLaM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)|[ChatGPT](https://openai.com/blog/chatgpt)|Galactica|GPT-NeoX<br>GPT Neo||RL-CAI|
2021|[LaMDA](https://blog.google/technology/ai/lamda/)|||GPT-J||||
--->

||2021|2022|2023|
:-:|:-:|:-:|:-:
Google|[LaMDA](https://blog.google/technology/ai/lamda/)|[GLaM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html), [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)|[Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/), [PaLM-2](https://ai.google/discover/palm2)|
[OpenAI](https://openai.com/)||[ChatGPT](https://openai.com/blog/chatgpt)|[GPT-4](https://openai.com/product/gpt-4), [GPT-4 Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)|
MetaAI||Galactica|[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)<br>[LLaMA2](https://ai.meta.com/llama/), [HF](https://huggingface.co/blog/llama2)|
EleutherAI|GPT-J|GPT-NeoX,<br>GPT Neo|[Pythia](https://github.com/EleutherAI/pythia)|
[Stability AI](https://stability.ai/)|||[Stable Vicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot), [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models),<br>[Stable LM 3B](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices), [Stable Beluga](https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models),<br>[Stable Chat](https://stability.ai/blog/stable-chat-research-defcon-ai-village)|
Anthropic||RL-CAI|[Claude](https://www.anthropic.com/product), [Claude2](claude.ai)|
BigScience||[Bloom](https://huggingface.co/bigscience/bloom)||
DeepMind||Chinchilla||
Stanford|||[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)|
[Berkeley-BAIR](https://bair.berkeley.edu/)|||[Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)|
[Vicuna Team](https://lmsys.org/about/)|||[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)|
[TII](https://www.tii.ae/)|||[Falcon](https://huggingface.co/blog/falcon)|

- [Phind](https://www.phind.com/blog/phind-model-beats-gpt4-fast) - model that matches and exceeds GPT-4's coding abilities while running 5x faster
- [Mistral](https://mistral.ai/news/announcing-mistral-7b/) - a 7.3B parameter model easy to fine-tune on any task
- [phi-1.5](https://huggingface.co/microsoft/phi-1_5) - a 1.3 billion parameter model trained on 30 billion tokens. The dataset consists of "textbook-quality" synthetically generated data
- [FacTool](https://github.com/GAIR-NLP/factool) - a tool augmented framework for detecting factual errors of texts generated by LLMs. Factool now supports 4 tasks: knowledge-based QA, code generation, mathematical reasoning, scientific literature review
- [Nougat](https://facebookresearch.github.io/nougat/) - Neural Optical Understanding for Academic Documents, a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents, by MetaAI
- [TextFX](https://textfx.withgoogle.com/) - AI-powered tools for rappers, writers and wordsmiths
- [Prompt2Model](https://github.com/neulab/prompt2model) - a system that takes a natural language task description (like the prompts used for LLMs such as ChatGPT) to train a small special-purpose model that is conducive for deployment
- [Giraffe](https://blog.abacus.ai/blog/2023/08/22/giraffe-long-context-llms/) - a new family of models that are finetuned from base LLaMA and LLaMA2
- [ToolBench](https://github.com/OpenBMB/ToolBench) - open-source, large-scale, high-quality instruction tuning SFT data to facilitate the construction of powerful LLMs with general tool-use capability
- [Platypus](https://platypus-llm.github.io/) - a family of fine-tuned and merged LLMs that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work
- [OpenFlamingo V2](https://laion.ai/blog/open-flamingo-v2/) - an open-source effort to replicate DeepMind's Flamingo models
- [MetaGPT](https://github.com/geekan/metagpt) - a framework involving LLM-based multi-agents that encodes human standardized operating procedures (SOPs) to extend complex problem-solving capabilities that mimic efficient human workflows
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/)
- [FlashAttention](https://princeton-nlp.github.io/flash-atttention-2/) - an algorithm to speed up attention and reduce its memory footprint‚Äîwithout any approximation
- [Quivr](https://github.com/StanGirard/quivr) - utilizes the power of Generative AI to store and retrieve unstructured information
- [LongLLaMA](https://github.com/CStanKonrad/long_llama) - a LLM capable of handling long contexts of 256k tokens or even more
- [OpenLLaMA](https://github.com/openlm-research/open_llama) - open source reproduction of Meta AI‚Äôs LLaMA
- [BuboGPT](https://bubo-gpt.github.io/) - is an advanced LLM that incorporates multi-modal inputs including text, image and audio, with a unique ability to ground its responses to visual objects
- [LAION](https://laion.ai/) - Large-scale Artificial Intelligence Open Network
- [Dalai](https://cocktailpeanut.github.io/dalai/#/), [Code](https://github.com/cocktailpeanut/dalai) - run LLaMA and Alpaca on your computer
- [LLaMAChat](https://llamachat.app/) - allows you to chat with LLaMa, Alpaca and GPT4All models all running locally on your CPU
- [GPT4All](https://gpt4all.io/index.html), [Code](https://github.com/nomic-ai/gpt4all) - an open-source assistant-style LLM that run locally on your CPU
- [SdkVercelAI](https://play.vercel.ai/) - you can input a prompt, pick different LLMS, and compare two side by side
- [Pi](https://heypi.com/talk) - this bot is designed to be more of a personal assistant
- [ChatwithData.ai](https://chatwithdata.ai/) - AI tool that lets you extract valuable insights and information from data files effortlessly
- [Open Assistant](https://open-assistant.io/) - a completely open-source ChatGPT alternative
- [HuggingChat](https://huggingface.co/chat/) - first open-source alternative to ChatGPT Powered by Open Assistant's latest model
- [ChatPDF](https://www.chatpdf.com/) - chat with any PDF
- [PdfGPT](https://pdfgpt.io/) - is a tool where you can upload pdf and get summaries, answers to your questions by OpenAI
- [Baize](https://github.com/project-baize/baize-chatbot) - is an open-source chat model trained with LoRA. It uses 100k dialogs generated by letting ChatGPT chat with itself
- [Chameleon](https://chameleon-llm.github.io/) - is a compositional reasoning framework designed to enhance LLMs and overcome their inherent limitations, such as outdated information and lack of precise reasoning
- [LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory) - an open-source toolkit for pre-training, fine-tuning and deployment of Large Language Models (LLMs) and mutlimodal LLMs
- [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) - a lightweight adaption method for fine-tuning Instruction-following and Multi-modal LLaMA models

## Large Visual Language Models (LVLMs)
- [Qwen-VL](https://github.com/QwenLM/Qwen-VL) - is the multimodal version of the large model series. Accepts image, text, and bounding box as inputs, outputs text and bounding box
- [AnomalyGPT](https://github.com/CASIA-IVA-Lab/AnomalyGPT) - the LVLM based Industrial Anomaly Detection (IAD) method that can detect anomalies in industrial images without the need for manually specified thresholds
- [IDEFICS](https://huggingface.co/blog/idefics) - an open-access VLM based on Flamingo. The model accepts arbitrary sequences of image and text inputs and produces text outputs, aiming to bring transparency to AI systems and serve as a foundation for open research in multimodal AI systems
- [Prismer](https://shikun.io/projects/prismer) - a data- and parameter-efficient VLM that leverages an ensemble of diverse, pre-trained domain experts
- [MiniGPT-4](https://minigpt-4.github.io/) - upload an image, and then use chat to identify what's in the picture and learn more about it
- [MultiModal-GPT](https://github.com/open-mmlab/Multimodal-GPT) - a vision and language model for multi-round dialogue with humans; the model is fine-tuned from OpenFlamingo, with LoRA added in the cross-attention and self-attention parts of the language model
- [LLaVA](https://llava-vl.github.io/) - a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding
- [TaskMatrix](https://github.com/microsoft/visual-chatgpt) - connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting

## Evaluation
- [Arthur Bench](https://www.arthur.ai/blog/introducing-arthur-bench) - an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models
- [AgentBench](https://github.com/THUDM/AgentBench) - the first benchmark designed to evaluate LLM-as-Agent across a diverse spectrum of different environments
- [L-Eval](https://github.com/OpenLMLab/LEval) - a comprehensive long-context language models evaluation suite with 18 long document tasks across multiple domains that require reasoning over long texts, including summarization, question answering, in-context learning with long CoT examples, topic retrieval, and paper writing assistance
- [OpenICL](https://github.com/Shark-NLP/OpenICL) - an open-source toolkit for in-context learning and LLM evaluation; supports various state-of-the-art retrieval and inference methods, tasks, and zero-/few-shot evaluation of LLMs
- [Open LLM Leaderboard: HF](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) - aims to track, rank and evaluate LLMs and chatbots as they are released
- [Chatbot Arena](https://chat.lmsys.org/) - a scalable and gamified evaluation of LLMs via crowdsourcing and Elo rating systems. Chat with two anonymous models side-by-side and vote for which one is better
- [OpenAGI](https://github.com/agiresearch/OpenAGI) - an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models

## Libraries
<!---
- [![GitHub Repo stars](https://img.shields.io/github/stars/hwchase17/langchain?style=socail)](https://github.com/hwchase17/langchain)
--->
- [LangChain](https://github.com/hwchase17/langchain), [docs](https://python.langchain.com/docs/get_started/introduction.html) - a framework for developing applications powered by language models
- [LlamaIndex](https://github.com/jerryjliu/llama_index), [docs](https://gpt-index.readthedocs.io/en/latest/) - a ‚Äúdata framework‚Äù to help you build LLM apps
- [outlines](https://github.com/outlines-dev/outlines), [docs](https://outlines-dev.github.io/outlines/) - a Python library to write reliable programs for interactions with generative models: language models, diffusers, multimodal models, classifiers, etc
- [guidance](https://github.com/guidance-ai/guidance) - interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text
- [agents](https://github.com/aiwaves-cn/agents) - an open-source library/framework for building autonomous language agents
- [nanoGPT](https://github.com/karpathy/nanoGPT) - the simplest, fastest repository for training/finetuning medium-sized GPTs
- [TorchScale](https://github.com/microsoft/torchscale) - a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively
- [InvokeAI](https://invoke-ai.github.io/InvokeAI/) - an implementation of Stable Diffusion, the open source text-to-image and image-to-image generator
- [ComfyUI](https://github.com/comfyanonymous/ComfyUI) - a powerful and modular stable diffusion GUI and backend. This UI will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface
- [StableSwarmUI](https://github.com/Stability-AI/StableSwarmUI) - Modular Stable Diffusion Web-User-Interface, with an emphasis on making powertools easily accessible, high performance, and extensibility
- [Wanda](https://github.com/locuslab/wanda) - Pruning LLMs by Weights and Activation: removes weights on a per-output basis, by the product of weight magnitudes and input activation norms
- [LOMO: LOw-Memory Optimization](https://github.com/OpenLMLab/LOMO) - a new optimizer, which fuses the gradient computation and the parameter update in one step to reduce memory usage
- [LMFlow](https://github.com/OptimalScale/LMFlow) - an extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community
- [Heron](https://github.com/turingmotors/heron) - a library that seamlessly integrates multiple Vision and Language models, as well as Video and Language models. Additionally, we provide pretrained weights trained on various datasets
- [Curated Transformers](https://github.com/explosion/curated-transformers) - is a transformer library for PyTorch. It provides state-of-the-art models that are composed from a set of reusable components, by Explosion
- [spacy-llm](https://github.com/explosion/spacy-llm) - integrates LLMs into spaCy, featuring a modular system for fast prototyping and prompting, and turning unstructured responses into robust outputs for various NLP tasks, no training data required, by Explosion
- [Medusa](https://github.com/FasterDecoding/Medusa) - a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads

## Tools
|Text-to-Image|Text-to-Music|Text-to-Video|Games|Brand|Prompt|
:-:|:-:|:-:|:-:|:-:|:-:
[Midjourney](https://www.midjourney.com/)|[Mubert](https://mubert.com/)|[GENMO](https://alpha.genmo.ai/)|[Leonardo.Ai](https://leonardo.ai/) - Assets|[Flair](https://flair.ai/)|[G-prompter](https://www.g-prompter.com/en)|
[Adobe Firefly](https://firefly.adobe.com/)|[Waveformer](https://waveformer.replicate.dev/)|[PIKA LABS](https://www.pika.art/)|[Dreamlab](https://dreamlab.gg/) - Animated Sprites|[Logolivery](https://logolivery.ai/)|[Prompt Builder](https://promptomania.com/midjourney-prompt-builder/)|
[Catbird](https://www.catbird.ai/)||[Kaiber](https://kaiber.ai/)|[Didimo](https://www.didimo.co/)||[Midjourney PromptHelper1](https://promptfolder.com/midjourney-prompt-helper/)|
[BlueWillow](https://www.bluewillow.ai/)||[Invidio](https://invideo.io/)|[Scenario](https://www.scenario.com/) - Assets||[Midjourney PromptHelper2](https://midjourney-prompt-helper.netlify.app/)|
[Lexica](https://lexica.art/)||[Moonvalley](https://moonvalley.ai/)|[Skybox](https://skybox.blockadelabs.com/) - World-building|
[Playground](https://playgroundai.com/)||[Morph Studio](https://www.morphstudio.com/)|[lumine AI](https://ilumine.ai/)|
|[Imgcreator](https://imgcreator.zmo.ai/)|
[Craiyon](https://www.craiyon.com/)||||

## Text-to-image

||Models|
:-:|:-:
|OpenAI|[CLIP](https://openai.com/research/clip), [DALL¬∑E](https://openai.com/research/dall-e), [DALL¬∑E 2](https://openai.com/dall-e-2), [DALL¬∑E 3](https://openai.com/dall-e-3)|
|Google|[Muse](https://muse-model.github.io/), [Imagen](https://imagen.research.google/), [Parti](https://sites.research.google/parti/)<br>[HyperDreamBooth](https://hyperdreambooth.github.io/), [DreamBooth](https://dreambooth.github.io/)<br>[StyleDrop](https://styledrop.github.io/)|
|MetaAI|[CM3leon](https://ai.meta.com/blog/generative-ai-text-images-cm3leon)|
|stability.ai|[Stable Diffusion XL](https://stability.ai/stable-diffusion), [DreamStudio](https://dreamstudio.ai/), [Clipdrop](https://clipdrop.co/)<br>[DeepFloyd IF](https://stability.ai/blog/deepfloyd-if-text-to-image-model): ([Code](https://github.com/deep-floyd/IF), [Demo: HF](https://huggingface.co/spaces/DeepFloyd/IF))|

- [OpenCLIP](https://github.com/mlfoundations/open_clip) - an open source implementation of OpenAI's CLIP (Contrastive Language-Image Pre-training)
- [LEDITS](https://editing-images-project.hf.space/) - combined lightweight approach for real-image editing, incorporating the Edit Friendly DDPM inversion technique with Semantic Guidance, thus extending Semantic Guidance to real image editing, while harnessing the editing capabilities of DDPM inversion
- [W√ºrstchen](https://huggingface.co/blog/wuerstchen) - Fast Diffusion for Image Generation
- [ExactlyAI](https://exactly.ai) - create images in seconds with an AI that understands your style
- [ConceptLab](https://kfirgoldberg.github.io/ConceptLab/) - generative models have enabled us to transform our words into vibrant, captivating imagery
- [IP-Adapter](https://ip-adapter.github.io/) - Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models
- [MATCHAI](https://www.color.io/) - a powerful web app that can copy the color grading from images so you can apply it to your own
- [Ideogram](https://ideogram.ai/launch) - AI tools that will make creative expression more accessible, fun, and efficient
- [Picogen](https://picogen.io/) - nonofficial API to Midjourney AI, Stability AI and DALLE-2 AI
- [FABRIC](https://sd-fabric.github.io/) - Feedback via Attention-Based Reference Image Conditioning - a technique to incorporate iterative feedback into the generative process of diffusion models based on StableDiffusion.
- [Controlling Text-to-Image Diffusion by Orthogonal Finetuning (OFT)](https://oft.wyliu.com/) - for adapting text-to-image diffusion models to downstream tasks
- [InstructPix2Pix Learning to Follow Image Editing Instructions](https://www.timothybrooks.com/instruct-pix2pix/) - a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image
- [Composer](https://damo-vilab.github.io/composer-page/) - a large (5 billion parameters) controllable diffusion model trained on billions of (text, image) pairs. It can exponentially expand the control space through composition, leading to an enormous number of ways to generate and manipulate images, i.e., making the infinite use of finite means
- [GigaGAN: Large-scale GAN for Text-to-Image Synthesis](https://mingukkang.github.io/GigaGAN/) - changing texture with prompting, changing style with prompting

## Multi-modal
- [Pinokio](https://pinokio.computer/) - a browser that lets you install, run, and programmatically control ANY application, automatically
- [ImageBind](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/), [Demo](https://imagebind.metademolab.com/demo), [Code](https://github.com/facebookresearch/ImageBind) - Image->Audio, Audio->Image, Text->Image&Audio, Aidio&Image->Image, Audio->Generated Image, by MetaAI
- [GEN-1](https://runwayml.com/ai-magic-tools/gen-1/), [Research](https://research.runwayml.com/gen1) - use words and images to generate new videos out of existing ones by Runway: [AI-Magic-Tools](https://runwayml.com/ai-magic-tools/)
- [GEN-2](https://runwayml.com/ai-magic-tools/gen-2/), [Research](https://research.runwayml.com/gen2) - create videos in any style you can imagine with Text to Video generation by Runway: [AI-Magic-Tools](https://runwayml.com/ai-magic-tools/)
  - Mode 01: Text to Video: Synthesize videos in any style you can imagine using nothing but a text prompt. If you can say it, now you can see it
  - Mode 02: Text + Image to Video: Generate a video using a driving image and a text prompt
  - Mode 03: Image to Video: Generate video using just a driving image (Variations Mode)
  - Mode 04: Stylization: Transfer the style of any image or prompt to every frame of your video
  - Mode 05: Storyboard: Turn mockups into fully stylized and animated renders
  - Mode 06: Mask: Isolate subjects in your video and modify them with simple text prompts
  - Mode 07: Render: Turn untextured renders into realistic outputs by applying an input image or prompt
  - Mode 08: Customization: Unleash the full power of Gen-2 by customizing the model for even higher fidelity results
- [MONSTER API](https://monsterapi.ai/)
  - text-to-image: a latent text-to-image diffusion model capable of generating photo-realistic images conditioned on text descriptions
  - image-to-image: a latent diffusion model capable of generating photo-realistic generating image-to-image translations guided by a text prompt
  - instruct-pix2pix: a model enables fast and effective image editing based on simple instructions

## Images
- [Stable Signature](https://ai.meta.com/blog/stable-signature-watermarking-generative-ai/) - a new method for watermarking images, by MetaAI
- [wasitai](https://wasitai.com/) - check if an image was generated by a machine
- [Textify](https://textify.storia.ai/) - a tool for replacing the gibberish in AI-generated images with your desired text
- [Interpolating between Images with Diffusion Models](https://clintonjwang.github.io/interpolation) - a method for zero-shot controllable interpolation using latent diffusion models
- [AnyDoor: Zero-shot Object-level Image Customization](https://damo-vilab.github.io/AnyDoor-Page/) - a diffusion-based image generator with the power to move target objects to new scenes at user-specified locations in a harmonious way
- [Matting Anything](https://chrisjuniorli.github.io/project/Matting-Anything/), [Code](https://github.com/SHI-Labs/Matting-Anything), [Demo: HF](https://huggingface.co/spaces/shi-labs/Matting-Anything) - an efficient and versatile framework for estimating the alpha matte of any instance in an image with user-prompt guidance
- [Plug-and-Play](https://pnp-diffusion.github.io/), [Code](https://github.com/MichalGeyer/plug-and-play) - a large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, allowing us to synthesize diverse images that convey highly complex visual concepts
- [Real-Time Neural Appearance Models](https://research.nvidia.com/labs/rtr/neural_appearance_models/) - a complete system for real-time rendering of scenes with complex appearance previously reserved for offline use, by NVIDIA
- [Designer](https://designer.microsoft.com/), [Microsoft Designer expands preview with new AI design features](https://www.microsoft.com/en-us/microsoft-365/blog/2023/04/27/microsoft-designer-expands-preview-with-new-ai-design-features/) by Microsoft. Designer has all the tools you‚Äôd expect, plus a few AI superpowers. Generate stunning designs and original images just by typing what you want. Get writing assistance and automatic layout suggestions for anything you add. Designer can even propose captions and hashtags to make social media sharing effortless
- [Scribble Diffusion](https://scribblediffusion.com/) - turn your sketch into a refined image using AI
- [StudioGPT](https://www.latentlabs.art/) - a tool for reimagining an existing image

## Computer Vision
- [PUG (Photorealistic Unreal Graphics)](https://pug.metademolab.com/) - 3 datasets for representation learning research
- [FACET (FAirness in Computer Vision EvaluaTion)](https://ai.meta.com/blog/dinov2-facet-computer-vision-fairness-evaluation) - a new comprehensive benchmark for evaluating the fairness of computer vision models across classification, detection, instance segmentation, and visual grounding tasks
- [Tracking Anything in High Quality](https://github.com/jiawen-zhu/HQTrack) - a framework for high performance video object tracking and segmentation
- [DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data](https://dreamsim-nights.github.io/) -  a new benchmark of synthetic image triplets that span a wide range of mid-level variations, labeled with human similarity judgments
- [CoTracker](https://co-tracker.github.io/) - an architecture that jointly tracks multiple points throughout an entire video, by MetaAI
- [TAPIR](https://deepmind-tapir.github.io/) - a model for Tracking Any Point (TAP) that effectively tracks a query point in a video sequence, by Google DeepMind
- [DreamTeache](https://research.nvidia.com/labs/toronto-ai/DreamTeacher/) - a self-supervised feature representation learning framework that utilizes generative networks for pre-training downstream image backbones, by NVIDIA
- [I-JEPA](https://ai.facebook.com/blog/yann-lecun-ai-model-i-jepa), [Code](https://github.com/facebookresearch/ijepa) - Image Joint Embedding Predictive Architecture is a method for self-supervised learning. At a high level, I-JEPA predicts the representations of part of an image from the representations of other parts of the same image
- [Visual Prompting](https://landing.ai/What-is-visual-prompting/) - an innovative approach that takes text prompting, used in applications such as ChatGPT, to computer vision
- [Tracking Everything Everywhere All at Once](https://omnimotion.github.io/) - a new test-time optimization method for estimating dense and long-range motion from a video sequence
- [Track-Anything](https://github.com/gaomingqi/Track-Anything) - a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only
- [Segment Anything Model (SAM)](https://segment-anything.com/) - a new AI model from MetaAI that can "cut out" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training. [Blog: Introducing Segment Anything](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/), [Code](https://github.com/facebookresearch/segment-anything)
- [DINOv2](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/) - a new method for training high-performance CV models, state-of-the-art CV models with self-supervised learning
- [Behind the Scenes: Density Fields for Single View Reconstruction](https://fwmb.github.io/bts/) - a neural network that predicts an implicit density field from a single image

## Video & Animation
- [Hotshot](https://hotshot.co/) - GIF generator
- [Unscreen](https://www.unscreen.com/) - remove video background
- [Motrica](https://www.motorica.ai/) - technologies and tools for advanced character animation
- [CoDeF](https://qiuyu96.github.io/CoDeF/) - Content Deformation Fields for Temporally Consistent Video Processing
- [MagicEdit](https://magic-edit.github.io/) - supports various editing applications, including video stylization, local editing, video-MagicMix and video outpainting
- [To Infinity and Beyond](https://fablestudio.github.io/showrunner-agents/) - an approach to generating high-quality episodic content for IP's (Intellectual Property) using LLMs, custom state-of-the art diffusion models and our multi-agent simulation for contextualization, story progression and behavioral control
- [PlazmaPunk](https://www.plazmapunk.com/) - create your own music video with the power of AI
- Video-LLaMA, [Code](https://github.com/DAMO-NLP-SG/Video-LLaMA), [Demo: HF](https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA) - a multi-model LLM that achieves video-grounded conversations between humans and computers by connecting language decoder with off-the-shelf unimodal pre-trained models
- [AnimateDiff prompt travel](https://github.com/s9roll7/animatediff-cli-prompt-travel) - AnimateDiff with prompt travel + ControlNet + IP-Adapter
- [AnimateDiff](https://animatediff.github.io/), [Code](https://github.com/guoyww/AnimateDiff) - Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning
- [Animate-A-Story](https://videocrafter.github.io/Animate-A-Story/) - a video storytelling approach which can synthesize high-quality, structure-controlled, and character-controlled videos
- [Zeroscope](https://huggingface.co/spaces/fffiloni/zeroscope) - a watermark-free Modelscope-based video model optimized for producing high-quality 16:9 compositions and a smooth video output
- [Klap](https://klap.app/) - a tool that analyzes the video and finds short clips
- [Lalamu](https://lalamu.studio/demo) - low-quality video lip sync with preselected videos/video templates (take clips from videos, give the video new audio, and then the lips will sync up to that new audio within the video)
- [D-ID](https://www.d-id.com/) - uses generative AI to create customized videos featuring talking avatars at a touch of a button for businesses and creators.
- [Rooms.xyz](https://rooms.xyz) - create & remix interactive rooms from your browser
- [Wonder Dynamics](https://wonderdynamics.com/) - an AI tool that automatically animates, lights, and composes CG characters into a live-action scene
- [REVELxyz](https://www.revel.xyz/animai) - a tool for creating Animated Avatars from a single photo
- [ANIMATED DRAWINGS](https://sketch.metademolab.com/) - a tool that brings children's drawings to life, by animating characters to move around, by MetaAI
- [RERENDER A VIDEO](https://anonymous-31415926.github.io/), [Demo: HF](https://huggingface.co/spaces/Anonymous-sub/Rerender) - a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos
- Roop, [Code](https://github.com/s0md3v/roop) - take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training
- [Text2Performer](https://yumingj.github.io/projects/Text2Performer.html) - Text-Driven Human Video Generation, where a video sequence is synthesized from texts describing the appearance and motions of a target performer
- [DragGAN](https://vcai.mpi-inf.mpg.de/projects/DragGAN/), [Code](https://github.com/XingangPan/DragGAN), [Demo: HF](https://huggingface.co/spaces/DragGan/DragGan) - way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc
- [DragDiffusion](https://yujun-shi.github.io/projects/dragdiffusion.html) - Harnessing Diffusion Models for Interactive Point-based Image Editing
- [In-N-Out: Face Video Inversion and Editing with Volumetric Decomposition](https://in-n-out-3d.github.io/) - our core idea is to represent the face in a video using two neural radiance fields, one for in-distribution and the other for out-of-distribution data, and compose them together for reconstruction
- [High-Resolution Video Synthesis with Latent Diffusion Models](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) - Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space, by NVIDIA

## 3D
 - [DreamCraft3D](https://mrtornado24.github.io/DreamCraft3D/) - a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects
 - [Genie](https://lumalabs.ai/genie) - 3D fundational model, by [Lumalabs](https://lumalabs.ai/)
 - [Masterpiece X](https://www.masterpiecex.com/) - the generative text-to-3D app that allows users to create 3D objects and characters complete with mesh, texture, and animations
 - [GAUSSIAN SPLAT](https://poly.cam/gaussian-splatting) - a rasterization technique for 3D reconstruction and rendering
 - [SyncDreamer](https://github.com/liuyuan-pal/SyncDreamer) - generating multiview-consistent images from a single-view image
 - [MAV3D (Make-A-Video3D)](https://make-a-video3d.github.io/) - a method for generating three-dimensional dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural Radiance Field (NeRF), which is optimized for scene appearance, density, and motion consistency by querying a Text-to-Video (T2V) diffusion-based model
 - [HiFA](https://hifa-team.github.io/HiFA-site/) - High-fidelity Text-to-3D with Advanced Diffusion Guidance
 - [AutoRecon](https://zju3dv.github.io/autorecon/) - a framework named for the automated discovery and reconstruction of an object from multi-view images
 - [BITE](https://bite.is.tue.mpg.de/) - enables 3D shape and pose estimation of dogs from a single input image. The model handles a wide range of shapes and breeds, as well as challenging postures far from the available training poses, like sitting or lying on the ground
 - [CSM (Common Sense Machines)](https://3d.csm.ai/) - generate your own textured 3D assets
 - [MotionGPT: Human Motion as Foreign Language](https://motion-gpt.github.io/) - a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks
 - [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360¬∞](https://sizhean.github.io/panohead) - the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360¬∞ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training
 - [AvatarBooth](https://zeng-yifei.github.io/avatarbooth_page/) - a text-to-3D model. It creates an animatable 3D model with your word description. Also, it can generate customized model with 4~6 photos from your phone or a character design generated from diffusion model
 - [Infinigen](https://infinigen.org/), [Code](https://github.com/princeton-vl/infinigen) - a procedural generator of 3D scenes, creating depth maps and labeling every aspect of the world it generates, by Princeton Vision & Learning Lab
 - [USD - Universal Scene Description](https://developer.nvidia.com/usd) - an open and extensible framework and ecosystem for describing, composing, simulating and collaborating within 3D worlds, originally developed by Pixar Animation Studios
 - [Shap-E: Demo](https://huggingface.co/spaces/hysts/Shap-E), [Code](https://github.com/openai/shap-e) - a conditional generative model for 3D assets, by OpenAI
 - [Neural Kernel Surface Reconstruction](https://research.nvidia.com/labs/toronto-ai/NKSR/), [Code](https://github.com/nv-tlabs/nksr)- a novel method for reconstructing a 3D implicit surface from a large-scale, sparse, and noisy point, by NVIDIA
 - [Neuralangelo](https://research.nvidia.com/labs/dir/neuralangelo/) - a framework for high-fidelity 3D surface reconstruction from RGB video captures. Using ubiquitous mobile devices, we enable users to create digital twins of both object-centric and large-scale real-world scenes with highly detailed 3D geometry, by NVIDIA
 - [Rodin Diffusion](https://3d-avatar-diffusion.microsoft.com/) - a Generative Model for Sculpting 3D Digital Avatars, by Microsoft
 - [3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) - three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (‚â• 100 fps) novel-view synthesis at 1080p resolution
 - [ConsistentNeRF](https://skhu101.github.io/ConsistentNeRF) - a method that leverages depth information to regularize both multi-view and single-view 3D consistency among pixels
 - [Text2NeRF](https://eckertzhang.github.io/Text2NeRF.github.io/) - a text-driven 3D scene generation framework, combines the neural radiance field (NeRF) and a pre-trained text-to-image diffusion model to generate diverse view-consistent indoor and outdoor 3D scenes from natural language descriptions
 - [Zip-NeRF](https://jonbarron.info/zipnerf/) - a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP
 - [S-NeRF](https://ziyang-xie.github.io/s-nerf/) - a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly
 - [Mip-NeRF 360](https://jonbarron.info/mipnerf360/) - Unbounded Anti-Aliased Neural Radiance Fields, an extension of mip-NeRF that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes
 - [3D-aware Conditional Image Synthesis](https://www.cs.cmu.edu/~pix2pix3D/) - a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model synthesizes a photo from different viewpoints
 - [Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior](https://make-it-3d.github.io/) - can create high-fidelity 3D content from only a single image
 - [Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](https://lukashoel.github.io/text-to-room/) - generates textured 3D meshes from a given text prompt using 2D text-to-image models
 - [OmniObject3D](https://omniobject3d.github.io/) - a large vocabulary 3D object dataset with massive high-quality real-scanned 3D objects to facilitate the development of 3D perception, reconstruction, and generation in the real world

## Audio & Speech & Music
- [StockMusic](https://www.stockmusic.app/) - a platform for AI-generated tunes that allows you to generate up to 10 minutes of copyright-free music
- [Stable Audio](https://stability.ai/blog/stable-audio-using-ai-to-generate-music) - a system that generates music and sound effects from text
- [RIFFUSION](https://www.riffusion.com/about) - the model to generate images of spectrograms and can then be converted to an audio clip
- [CLAP](https://github.com/LAION-AI/CLAP/) - you can extract a latent representation of any given audio and text for your own model, or for different downstream task
- [Vscoped](https://vscoped.com/) - effortlessly transcribe your video content to boost click-through rates and watch time
- [SeamlessM4T](https://ai.meta.com/blog/seamless-m4t/) - a foundational multilingual and multitask model that seamlessly translates and transcribes across speech and text: automatic speech recognition, speech-to-text and speech-to-speech translation, text-to-text and text-to-speech translation
- [Eleven Labs](https://beta.elevenlabs.io/) - the most realistic Text to Speech and Voice Cloning software
  - [Eleven Multilingual v2](https://elevenlabs.io/blog/multilingualv2/) - a Foundational AI Speech Model for Nearly 30 Languages
  - [Eleven Multilingual v1](https://beta.elevenlabs.io/blog/eleven-multilingual-v1/), [Demo](https://beta.elevenlabs.io/?ref=beta.elevenlabs.io) - generate top-quality spoken audio in any voice and style with the most advanced and multipurpose AI speech tool out there
  - [AI Speech Classifier](https://beta.elevenlabs.io/blog/ai-speech-classifier/), [Demo](https://beta.elevenlabs.io/ai-speech-classifier) - detect whether an audio clip was created using ElevenLabs
- [AudioCraft](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) - simple framework that generates high-quality, realistic audio and music from text-based user inputs after training on raw audio signals as opposed to MIDI or piano rolls, by MetaAI
  - [MusicGen](https://ai.honu.io/papers/musicgen/), [Demo: HF](https://huggingface.co/spaces/facebook/MusicGen), [Code](https://github.com/facebookresearch/audiocraft) - a simple and controllable model for music generation
  - [AudioGen](https://felixkreuk.github.io/audiogen/) - an auto-regressive generative model that generates audio samples conditioned on text inputs
  - [EnCodec](https://ai.meta.com/blog/ai-powered-audio-compression-technique/) - a neural network that is trained end to end to reconstruct the input signal
- [MuAViC](https://ai.facebook.com/blog/muavic-audio-visual-speech-translation-benchmark/) - a Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation, by MetaAI
- [Voicebox](https://voicebox.metademolab.com/) - Text-Guided Multilingual Universal Speech Generation at Scale, by MetaAI
- [AudioPaLM](https://google-research.github.io/seanet/audiopalm/examples/) - a large language model for speech understanding and generation, by Google
- MERT, [Code](https://github.com/yizhilll/MERT), [Demo: HF](https://huggingface.co/spaces/m-a-p/Music-Descriptor) - an Acoustic Music Understanding Model with Large-Scale Self-supervised Training
- [Ecoute](https://github.com/SevaSk/ecoute) - a live transcription tool that provides real-time transcripts for both the user's microphone input (You) and the user's speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI's GPT-3.5 for the user to say based on the live transcription of the conversation
- [SadTalker: Demo](https://huggingface.co/spaces/vinthony/SadTalker) - Stylized Audio-Driven Single Image Talking Face Animation
- [Recast](https://www.letsrecast.ai/) - turn your want-to-read articles into rich audio summaries
 - AudioGPT, [Demo: HuggingFace](https://huggingface.co/spaces/AIGC-Audio/AudioGPT), [Code](https://github.com/AIGC-Audio/AudioGPT) - Understanding and Generating Speech, Music, Sound, and Talking Head
 - [Chirp](https://suno-ai.notion.site/Chirp-Examples-f05351485da74d769d6183220a6e5da7) - music model, generates realistic audio - including speech, music and sound effects
 - [Bark](https://github.com/suno-ai/bark) - a transformer-based text-to-audio model created, by [Suno](https://www.suno.ai/). Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communication like laughing, sighing and crying
- [Whisper](https://openai.com/research/whisper) - an automatic speech recognition (ASR) system, that approaches human level robustness and accuracy on English speech recognition
- [MusicLM](https://google-research.github.io/seanet/musiclm/examples/), [Demo](https://aitestkitchen.withgoogle.com/experiments/music-lm) - a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff", by Google Research
- [Universal Speech Model (USM)](https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html) - a state-of-the-art speech AI for 100+ languages, by Google Research
- [Musicfy](https://www.musicfy.lol/) - music like you've never heard. Create and discover AI covers of your favorite songs
- [Jukebox](https://openai.com/research/jukebox) - learned to compress their training set and generated audio from this compressed space
- [Koe Recast](https://koe.ai/) - transform your voice using AI

## Code & Math
- [MAmmoTH](https://tiger-ai-lab.github.io/MAmmoTH/) -  a series of open-source LLMs specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset
- [Defog](https://defog.ai/blog/open-sourcing-sqlcoder/) - a state-of-the-art LLM for converting natural language questions to SQL queries, which outperforms major open-source models and slightly outperforms gpt-3
- [v0](https://v0.dev/) - a generative user interface system by Vercel Labs powered by AI. It generates copy-and-paste friendly React code based on Shadcn UI and Tailwind CSS that people can use in their projects
- [Open Interpreter](https://github.com/KillianLucas/open-interpreter) - an open-source, locally running implementation of OpenAI's Code Interpreter
- [SafeCoder](https://huggingface.co/blog/safecoder) - a code assistant solution built for the enterprise. In marketing speak: ‚Äúyour own on-prem GitHub copilot‚Äù, by Hugging Face
- [Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/) - a state-of-the-art LLM capable of generating code, and natural language about code, from both code and natural language prompts, by Meta AI
- [StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding) - LLM generative AI product for coding designed to assist programmers with their daily work, by Stability AI
- [Teaching Arithmetic to Small Transformers](https://github.com/lee-ny/teaching_arithmetic) - small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective
- [InterCode](https://intercode-benchmark.github.io/) - framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations
- [CodeGen2.5](https://blog.salesforceairesearch.com/codegen25/) -  LLMs for program synthesis, by Salesforce
- [LeanDojo](https://leandojo.org/) - set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research
- [GPT Engineer](https://github.com/AntonOsika/gpt-engineer) - is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt
- [CodeTF](https://github.com/salesforce/CodeTF) - a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on. It aims to facilitate easy integration of SOTA CodeLLMs into real-world applications
- [Let‚Äôs Verify Step by Step](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision#fn-1) - a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (‚Äúprocess supervision‚Äù) instead of simply rewarding the correct final answer (‚Äúoutcome supervision‚Äù), by OpenAI
- [ü¶ç Gorilla: LLM Connected with Massive APIs](https://gorilla.cs.berkeley.edu/) - a finetuned LLaMA-based model that surpasses GPT-4 on writing API calls
- [CodeT5 and CodeT5+](https://github.com/salesforce/CodeT5/tree/main) -  models can be deployed as an AI-powered coding assistant to boost the productivity of software developers, by Salesforce
- [Framer](https://www.framer.com/) - a tool that constructs a completely unique website for you based on a text prompt
- [Pico](https://picoapps.xyz/) - a tool that use GPT4 to instantly build simple, shareable web apps

## Games
- [AI Town](https://github.com/a16z-infra/AI-town) - a deployable starter kit for building and customizing your own version of AI town - a virtual town where AI characters live, chat and socialize.
- [Generative Agents: Interactive Simulacra of Human Behavior](https://github.com/joonspk-research/generative_agents) - contains our core simulation module for generative agents‚Äîcomputational agents that simulate believable human behaviors‚Äîand their game environment
- [STEVE-1](https://sites.google.com/view/steve-1) - a Generative Model for Text-to-Behavior in Minecraft
- [Mastering Stratego](https://www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information) - DeepNash, an AI agent that learned the game from scratch to a human expert level by playing against itself
- [Voyager: An Open-Ended Embodied Agent with LLMs](https://voyager.minedojo.org/) - the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention
  
## Robotics
- [Robot Parkour Learning](https://robot-parkour.github.io/) - a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data
- [Open X-Embodiment](https://robotics-transformer-x.github.io/) - Robotic Learning Datasets and RT-X Models
- [Eureka](https://eureka-research.github.io/) - a human-level reward design algorithm powered by LLMs, by NVIDIA
- [Language to rewards for robotic skill synthesis](https://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html) - an approach to teaching robots novel actions through natural language input is proposed, using reward functions as an interface to bridge the gap between language and low-level robot actions
- [VIMA](https://vimalabs.github.io/) - General Robot Manipulation with Multimodal Prompts
- [RT-2](https://robotics-transformer2.github.io/) - a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for robotic control
- [Robots That Ask For Help](https://robot-help.github.io/) - a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed
- [ViNT: A Foundation Model for Visual Navigation](https://visualnav-transformer.github.io/) - a goal-conditioned navigation policy trained on diverse, cross-embodiment training data, and can control many different robots in zero-shot
- [Navigating to Objects in the Real World](https://theophilegervet.github.io/projects/real-world-object-navigation/) - 
- [RVT: Robotic View Transformer](https://robotic-view-transformer.github.io/) - a multi-view transformer for 3D manipulation that is both scalable and accurate. RVT takes camera images and task language description as inputs and predicts the gripper pose action, by NVIDIA
- [TidyBot](https://tidybot.cs.princeton.edu/) - personalized Robot Assistance with Large Language Models
- [Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning](https://sites.google.com/view/op3-soccer) - by OP3 Soccer Team, DeepMind
- [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/) - embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts
- [Scaling Robot Learning with Semantically Imagined Experience](https://diffusion-rosie.github.io/) - 
- [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/) - low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface

## Typography
- [ControlNet](https://huggingface.co/DionTimmer/controlnet_qrcode), [Demo: HF](https://huggingface.co/spaces/huggingface-projects/QR-code-AI-art-generator), [How to make a QR code with Stable Diffusion](https://stable-diffusion-art.com/qr-code/) - QR Code Conditioned ControlNet Models for Stable Diffusion. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape
- [Word-As-Image for Semantic Typography](https://wordasimage.github.io/Word-As-Image-Page/) - A few examples of our Word-As-Image illustrations in various fonts and for different textual concept. The semantically adjusted letters are created completely automatically using our method, and can then be used for further creative design as we illustrate here
- [DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion](https://ds-fusion.github.io/) - create artistic typography automatically, a novel method to automatically generate artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word, while ensuring that the output remains readable

## Bio & Med
- [AlphaMissense](https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/) - an AI model classifying missense variants to help pinpoint the cause of diseases
- [evodiff](https://github.com/microsoft/evodiff) - combines evolutionary-scale data with diffusion models for controllable protein sequence generation
- [SAM-Med2D](https://github.com/OpenGVLab/SAM-Med2D) - applying the Segment Anything Model (SAM) to medical 2D images
- [Med-Flamingo](https://huggingface.co/med-flamingo/med-flamingo) - a medical vision-language model with multimodal in-context learning abilities
- [Brain2Music](https://google-research.github.io/seanet/brain2music/) - Reconstructing Music from Human Brain Activity
- [Seeing the World through Your Eyes](https://world-from-eyes.github.io/) - reconstruct a 3D scene beyond the camera's line-of-sight using portrait images containing eye reflections
- [Mind-Video](https://mind-video.com/) - High-quality Video Reconstruction from Brain Activity
- [Med-PaLM](https://sites.research.google/med-palm/) - a large language model (LLM) designed to provide high-quality answers to medical questions
- [PMC-LLaMA](https://github.com/chaoyi-wu/PMC-LLaMA) - the official codes for "PMC-LLaMA: Continue Training LLaMA on Medical Papers"
 
## Military
- [AIP Pillars](https://www.palantir.com/platforms/aip/) - activate LLMs and other AI on your private network, subject to full control

## Climat
- [ClimaX A foundation model for weather and climate](https://microsoft.github.io/ClimaX/) - a flexible and generalizable deep learning model for weather and climate science. [Introducing ClimaX: The first foundation model for weather and climate](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/introducing-climax-the-first-foundation-model-for-weather-and-climate/)

## Other: Fin, Presentation 
- [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)
- [guidde](https://www.guidde.com/) - create documentation/presentation/FAQ from captured video
- [Gamma](https://gamma.app/) - create visually appealing presentations
